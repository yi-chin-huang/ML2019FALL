{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import multiprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_x, train_y, test_x):\n",
    "    train_clean = clean_data(train_x['comment'])\n",
    "    test_clean = clean_data(test_x['comment'])\n",
    "    train_clean_drop = [s for s in train_clean if s != '']\n",
    "    test_clean_fill = [s if s != '' else \"good\" for s in test_clean]\n",
    "    train_label = [train_y['label'][i] for i, s in enumerate(train_clean) if s != '']\n",
    "    train_df_c = pd.DataFrame({'id': range(len(train_clean_drop)), 'comment': train_clean_drop})\n",
    "    test_df_c = pd.DataFrame({'id': range(len(test_clean_fill)), 'comment': test_clean_fill}) \n",
    "    corp = train_clean.copy()\n",
    "    corp.extend(test_clean)\n",
    "    return train_df_c, test_df_c, train_label, corp\n",
    "    \n",
    "def cleaning(doc):\n",
    "    txt = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    txt = [w for w in txt if w != ' ']\n",
    "    return ' '.join(txt)\n",
    "\n",
    "def clean_data(data):\n",
    "    nlp = spacy.load(\"en_core_web_sm\") \n",
    "    rm_user = (re.sub(\"@user\", '', str(row)).lower() for row in data)\n",
    "    rm_url = (re.sub(\"url\", '', str(row)).lower() for row in rm_user)\n",
    "    brief_cleaning = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in rm_url)\n",
    "    txt = [cleaning(doc) for doc in nlp.pipe(brief_cleaning, batch_size=5000, n_threads=-1)]\n",
    "    return txt\n",
    "\n",
    "def word2vec(corp):\n",
    "    cores = multiprocessing.cpu_count()\n",
    "    dictionary = [w.split() for w in corp]\n",
    "    w2v_model = Word2Vec(size=256, window=5, min_count=1, workers=cores)\n",
    "    w2v_model.build_vocab(dictionary) \n",
    "    w2v_model.train(dictionary, total_examples=w2v_model.corpus_count, epochs=20)\n",
    "    w2v_model.save(\"./model/train_word2vec_256.model\")\n",
    "    return w2v_model\n",
    "    \n",
    "def prepare_sequence(seq, word_to_ix):\n",
    "    seq = seq.split(' ')\n",
    "    idxs = [word_to_ix[w] for w in seq if w != ' ']\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "def make_dict(train_df_clean, test_df_clean):\n",
    "    word_to_ix = dict()\n",
    "    corpus = pd.concat([train_df_clean['comment'], test_df_clean['comment']], ignore_index=True)\n",
    "    for row in corpus:\n",
    "        sen = str(row).split(' ')\n",
    "        for word in sen:\n",
    "            if word not in word_to_ix:\n",
    "                word_to_ix[word] = len(word_to_ix)\n",
    "    return word_to_ix\n",
    "\n",
    "def build_pretrained_dict(words_dict, model):\n",
    "    dict_size = len(words_dict)\n",
    "    vec_dim = 256\n",
    "    tensor = torch.zeros([dict_size, vec_dim])\n",
    "    for word in words_dict:\n",
    "        idx = words_dict[word]\n",
    "        vec = model.wv[word]\n",
    "        tensor[idx,:] = torch.FloatTensor(vec)\n",
    "    return tensor\n",
    "\n",
    "class dataset(Dataset):\n",
    "    def __init__(self, train_x, train_y, word_to_ix):\n",
    "        self.train_x = train_x['comment']\n",
    "        self.label = train_y\n",
    "        self.w2v_model = w2v_model\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        sentence = self.train_x[index]\n",
    "        sentence = prepare_sequence(sentence, word_to_ix)\n",
    "        return torch.LongTensor(sentence), self.label[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.train_x.shape[0]\n",
    "    \n",
    "class dataset_test(Dataset):\n",
    "    def __init__(self, test_x, word_to_ix):\n",
    "        self.test_x = test_x['comment']\n",
    "    def __getitem__(self, index):\n",
    "        sentence = self.test_x[index]\n",
    "        sentence = prepare_sequence(sentence, word_to_ix)\n",
    "        return torch.LongTensor(sentence)\n",
    "    def __len__(self):\n",
    "        return self.test_x.shape[0]\n",
    "\n",
    "def add_padding(data):\n",
    "    sents = [s[0] for s in data]\n",
    "    labels = [s[1] for s in data]\n",
    "    sort_sents = sorted(sents, key=lambda x: len(x), reverse=True)\n",
    "    pad_sent = pad_sequence(sents, batch_first=True, padding_value=0)\n",
    "    return pad_sent, torch.LongTensor(labels)\n",
    "\n",
    "def add_padding_test(data):\n",
    "    sents = data\n",
    "    sort_sents = sorted(sents, key=lambda x: len(x), reverse=True)\n",
    "    pad_sent = pad_sequence(sents, batch_first=True, padding_value=0)\n",
    "    return pad_sent\n",
    "    \n",
    "class LSTMClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size, pretrained_vec):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)  \n",
    "        self.word_embeddings.weight.data.copy_(pretrained_vec)\n",
    "        self.word_embeddings.weight.requires_grad = False\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.hidden2class = nn.Sequential( \n",
    "            nn.Linear(4*hidden_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, tagset_size),\n",
    "            nn.Sigmoid(),\n",
    "        )        \n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        norm_lstm_out = lstm_out[:, -1, :].view(lstm_out.size(0), -1)\n",
    "        reverse_lstm_out = lstm_out[:, 0, :].view(lstm_out.size(0), -1)\n",
    "        lstm_out = torch.cat((norm_lstm_out, reverse_lstm_out), 1)\n",
    "        tag_space = self.hidden2class(lstm_out)\n",
    "        return tag_space\n",
    "    \n",
    "def train(model, train_dataloader, EPOCH):\n",
    "    \n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "    model = model.to(device)\n",
    "\n",
    "    if not os.path.exists(f'./hist_model'):\n",
    "        os.mkdir(f'./hist_model')\n",
    "\n",
    "    for epoch in range(1, 1+EPOCH):\n",
    "        model.train()\n",
    "        correct = 0\n",
    "        total_loss = 0\n",
    "        for batch_idx, (sentence, label) in enumerate(train_dataloader):\n",
    "            data, label = sentence.to(device), label.to(device)\n",
    "            optimizer.zero_grad() \n",
    "            output = model(data)\n",
    "            loss = loss_function(output, label)\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            predicted = torch.squeeze(predicted)\n",
    "            correct += (predicted == label).sum().item()    \n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(\"epoch :\", epoch)\n",
    "            accu = correct/len(train_loader.dataset)\n",
    "            print(\"TRAIN\", \"accuracy:\", accu, \"loss:\", total_loss)\n",
    "            torch.save(model.state_dict(), f'./hist_model/{epoch}')\n",
    "\n",
    "def predict(PATH, test_loader, pretrained_vec):\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "    pred_model = LSTMClassifier(256, 128, 16255, 2, pretrained_vec)\n",
    "    pred_model = pred_model.to(device)\n",
    "    pred_model.load_state_dict(torch.load(PATH))\n",
    "    pred_model.eval()\n",
    "    \n",
    "    prediction = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, sentence in enumerate(test_loader):\n",
    "            sentence = sentence.to(device)\n",
    "            out = pred_model(sentence)\n",
    "            _,pred_label = torch.max(out,1)\n",
    "            prediction.extend(pred_label.cpu().numpy())\n",
    "    return prediction\n",
    "\n",
    "def out(result, out_file):\n",
    "    df = pd.DataFrame({'id': np.arange(0,len(result)), 'label': result})\n",
    "    df.to_csv(out_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = pd.read_csv('./data/train_x.csv')\n",
    "train_y = pd.read_csv('./data/train_y.csv')\n",
    "test_x = pd.read_csv('./data/test_x.csv')\n",
    "train_df_c, test_df_c, train_label, corp = load_data(train_x, train_y, test_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix = make_dict(train_df_c, test_df_c)\n",
    "\n",
    "with open('./model/train_word_to_ix.pickle', 'wb') as file:\n",
    "    pickle.dump(word_to_ix, file)\n",
    "\n",
    "w2v_model = word2vec(corp)\n",
    "pretrained_vec = build_pretrained_dict(word_to_ix, w2v_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset(train_df_c, train_label, w2v_model)\n",
    "test_dataset = dataset_test(test_df_c, word_to_ix)\n",
    "train_loader = DataLoader(train_dataset, batch_size = 256, shuffle=False, collate_fn = add_padding)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 256, shuffle=False, collate_fn = add_padding_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 10\n",
      "TRAIN accuracy: 0.7199056460203926 loss: 30.065490126609802\n",
      "epoch : 20\n",
      "TRAIN accuracy: 0.760386546948714 loss: 28.278101593255997\n"
     ]
    }
   ],
   "source": [
    "model = LSTMClassifier(256, 128, len(word_to_ix), 2, pretrained_vec)\n",
    "EPOCH = 20\n",
    "train(model, train_loader, EPOCH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = f'./hist_model/{EPOCH}'\n",
    "if not os.path.exists('./result'):\n",
    "    os.mkdir('./result')\n",
    "prediction = predict(PATH, test_loader, pretrained_vec)\n",
    "output = out(prediction, f'./result/train_pred_{EPOCH}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16255\n"
     ]
    }
   ],
   "source": [
    "print(len(word_to_ix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_vec = build_pretrained_dict(word_to_ix, w2v_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16255, 256])\n"
     ]
    }
   ],
   "source": [
    "print(pretrained_vec.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_hw5",
   "language": "python",
   "name": "ml2019fall_hw5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
