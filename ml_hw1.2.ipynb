{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readdata(data):\n",
    "    \n",
    "    # 把有些數字後面的奇怪符號刪除\n",
    "    for col in list(data.columns[2:]):\n",
    "        data[col] = data[col].astype(str).map(lambda x: x.rstrip('x*#A'))\n",
    "    data = data.values\n",
    "    \n",
    "    # 刪除欄位名稱及日期\n",
    "    data = np.delete(data, [0,1], 1)\n",
    "\n",
    "    # 特殊值補 0\n",
    "    data[ data == 'NR'] = 0\n",
    "    data[ data == ''] = 0\n",
    "    data[ data == 'nan'] = 0\n",
    "    data = data.astype(np.float)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(data):\n",
    "    N = data.shape[0] // rows\n",
    "\n",
    "    temp = data[:18, :]\n",
    "    \n",
    "    # Shape 會變成 (x, 18) x = 取多少hours\n",
    "    for i in range(1, N):\n",
    "        temp = np.hstack((temp, data[i*18: i*18+18, :]))\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 8784)"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year1_pd = pd.read_csv('year1-data.csv')\n",
    "\n",
    "year1 = readdata(year1_pd)\n",
    "train_data = extract(year1)\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_std(data1, data2):\n",
    "    pm25 = np.concatenate((data1[9,:], data2[9,:]), axis=None)\n",
    "    pm25 = [i for i in pm25 if i > 2 and i <= 100]\n",
    "    pm25_mean = np.mean(pm25)\n",
    "    pm25_std = np.std(pm25)\n",
    "    return(pm25_mean,pm25_std)\n",
    "\n",
    "pm25mean, pm25std = get_mean_std(train_data1, train_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid2(x, y, mean, std, a):\n",
    "    lower = mean - a * std\n",
    "    upper = mean + a * std\n",
    "    if y <= lower or y >= upper:\n",
    "        return False\n",
    "    for i in range(9):\n",
    "        if x[9,i] <= lower or x[9,i] >= upper:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def parse2train2(data, pm25mean, pm25std, a):\n",
    "    x = []\n",
    "    y = []\n",
    "    # 用前面9筆資料預測下一筆PM2.5 所以需要-9\n",
    "    total_length = data.shape[1] - 9\n",
    "    for i in range(total_length):\n",
    "        x_tmp = data[:,i:i+9]\n",
    "        y_tmp = data[9,i+9]\n",
    "        if valid2(x_tmp, y_tmp, pm25mean, pm25std, a):\n",
    "            x.append(x_tmp.reshape(-1,))\n",
    "            y.append(y_tmp)\n",
    "    # x 會是一個(n, 18, 9)的陣列， y 則是(n, 1) \n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(x, y):\n",
    "    if y <= 2 or y > 100:\n",
    "        return False\n",
    "    for i in range(9):\n",
    "        if x[9,i] <= 2 or x[9,i] > 100:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def parse2train(data):\n",
    "    x = []\n",
    "    y = []\n",
    "    # 用前面9筆資料預測下一筆PM2.5 所以需要-9\n",
    "    total_length = data.shape[1] - 9\n",
    "    for i in range(total_length):\n",
    "        x_tmp = data[:,i:i+9]\n",
    "        y_tmp = data[9,i+9]\n",
    "        if valid(x_tmp, y_tmp):\n",
    "            x.append(x_tmp.reshape(-1,))\n",
    "            y.append(y_tmp)\n",
    "    # x 會是一個(n, 18, 9)的陣列， y 則是(n, 1) \n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch(x, y):\n",
    "    # 打亂data順序\n",
    "    index = np.arange(x.shape[0])\n",
    "    np.random.shuffle(index)\n",
    "    x = x[index]\n",
    "    y = y[index]\n",
    "    \n",
    "    # 訓練參數以及初始化\n",
    "    batch_size = 64\n",
    "    lr = 1e-3\n",
    "    lam = 0.001\n",
    "    beta_1 = np.full(x[0].shape, 0.9).reshape(-1, 1)\n",
    "    beta_2 = np.full(x[0].shape, 0.99).reshape(-1, 1)\n",
    "    w = np.full(x[0].shape, 0.1).reshape(-1, 1)\n",
    "    bias = 0.1\n",
    "    m_t = np.full(x[0].shape, 0).reshape(-1, 1)\n",
    "    v_t = np.full(x[0].shape, 0).reshape(-1, 1)\n",
    "    m_t_b = 0.0\n",
    "    v_t_b = 0.0\n",
    "    t = 0\n",
    "    epsilon = 1e-8\n",
    "    \n",
    "    for num in range(10):\n",
    "        for num in range(1000):\n",
    "            loss0 = np.full(batch_size, 0.0).reshape(-1, 1)\n",
    "            for b in range(int(x.shape[0]/batch_size)):\n",
    "                t+=1\n",
    "                x_batch = x[b*batch_size:(b+1)*batch_size]\n",
    "                y_batch = y[b*batch_size:(b+1)*batch_size].reshape(-1,1)\n",
    "                loss = y_batch - np.dot(x_batch,w) - bias\n",
    "\n",
    "                # 計算gradient\n",
    "                g_t = np.dot(x_batch.transpose(),loss) * (-2) +  2 * lam * np.sum(w)\n",
    "                g_t_b = loss.sum(axis=0) * (2)\n",
    "                m_t = beta_1*m_t + (1-beta_1)*g_t \n",
    "                v_t = beta_2*v_t + (1-beta_2)*np.multiply(g_t, g_t)\n",
    "                m_cap = m_t/(1-(beta_1**t))\n",
    "                v_cap = v_t/(1-(beta_2**t))\n",
    "                m_t_b = 0.9*m_t_b + (1-0.9)*g_t_b\n",
    "                v_t_b = 0.99*v_t_b + (1-0.99)*(g_t_b*g_t_b) \n",
    "                m_cap_b = m_t_b/(1-(0.9**t))\n",
    "                v_cap_b = v_t_b/(1-(0.99**t))\n",
    "                w_0 = np.copy(w)\n",
    "\n",
    "                # 更新weight, bias\n",
    "                w -= ((lr*m_cap)/(np.sqrt(v_cap)+epsilon)).reshape(-1, 1)\n",
    "                bias -= (lr*m_cap_b)/(math.sqrt(v_cap_b)+epsilon)\n",
    "                \n",
    "                loss0 += loss**2\n",
    "        rmse = np.sqrt(np.sum(loss0)/x.shape[0])\n",
    "        print('rmse:', rmse)        \n",
    "\n",
    "    return w, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_constant(x):\n",
    "    return(np.c_[x, np.ones(x.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x, y, l, i):\n",
    "    xss = x\n",
    "    yss = y\n",
    "    num_data, num_feature = xss.shape\n",
    "    ws = np.zeros(num_feature)\n",
    "    b = np.zeros(num_feature)\n",
    "    lr = l\n",
    "    it = i\n",
    "    prev_gra = np.zeros(num_feature)\n",
    "    for j in range(10):\n",
    "        for i in range(it):\n",
    "            predict = np.dot(xss,ws)\n",
    "            diffs = yss - predict\n",
    "            grad = np.dot(xss.transpose(),diffs) * (2)\n",
    "            prev_gra += grad**2\n",
    "            ada = np.sqrt(prev_gra)\n",
    "\n",
    "            ws = np.add(ws, lr * grad/ada)\n",
    "            rmse = np.sqrt(np.sum(diffs**2)/num_data)\n",
    "        print('iteration group:',j)\n",
    "        print('RMSE:',rmse)\n",
    "    return(ws, rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_dataset(trainxs, trainys):\n",
    "    # trainxs: tuple ((train_x1, train_x2))\n",
    "    train_x = np.vstack(trainxs)\n",
    "    train_y = np.hstack(trainys)\n",
    "    return train_x, train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse: 4.2045585941849755\n",
      "rmse: 4.198937646800971\n",
      "rmse: 4.196325779422934\n",
      "rmse: 4.194952336475901\n",
      "rmse: 4.194240611047877\n",
      "rmse: 4.1939521508697\n",
      "rmse: 4.193973246102989\n",
      "rmse: 4.194244641537489\n",
      "rmse: 4.19473470644956\n",
      "rmse: 4.195427832109967\n"
     ]
    }
   ],
   "source": [
    "year1_pd = pd.read_csv('year1-data.csv')\n",
    "\n",
    "year1 = readdata(year1_pd)\n",
    "train_data = extract(year1)\n",
    "train_x, train_y = parse2train(train_data)\n",
    "\n",
    "w, bias = minibatch(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### two years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "year1_pd = pd.read_csv('year1-data.csv')\n",
    "year2_pd = pd.read_csv('year2-data.csv')\n",
    "\n",
    "year1 = readdata(year1_pd)\n",
    "year2 = readdata(year2_pd)\n",
    "\n",
    "train_data1 = extract(year1)\n",
    "train_data2 = extract(year2)\n",
    "\n",
    "train_x1, train_y1 = parse2train(train_data1)\n",
    "train_x2, train_y2 = parse2train(train_data2)\n",
    "\n",
    "train_x0, train_y0 = expand_dataset((train_x1, train_x2), (train_y1, train_y2))\n",
    "train_x0 = add_constant(train_x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "year1_pd = pd.read_csv('year1-data.csv')\n",
    "year2_pd = pd.read_csv('year2-data.csv')\n",
    "\n",
    "year1 = readdata(year1_pd)\n",
    "year2 = readdata(year2_pd)\n",
    "\n",
    "train_data1 = extract(year1)\n",
    "train_data2 = extract(year2)\n",
    "\n",
    "train_x1, train_y1 = parse2train2(train_data1, pm25mean, pm25std, 3.6)\n",
    "train_x2, train_y2 = parse2train2(train_data2, pm25mean, pm25std, 3.6)\n",
    "\n",
    "train_x, train_y = expand_dataset((train_x1, train_x2), (train_y1, train_y2))\n",
    "train_x1 = add_constant(train_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration group: 0\n",
      "RMSE: 4.466217307382477\n",
      "iteration group: 1\n",
      "RMSE: 4.435550974367724\n",
      "iteration group: 2\n",
      "RMSE: 4.428448614147988\n",
      "iteration group: 3\n",
      "RMSE: 4.425178004478284\n",
      "iteration group: 4\n",
      "RMSE: 4.423111059632232\n",
      "iteration group: 5\n",
      "RMSE: 4.421586910885842\n",
      "iteration group: 6\n",
      "RMSE: 4.420358764978588\n",
      "iteration group: 7\n",
      "RMSE: 4.419313066523533\n",
      "iteration group: 8\n",
      "RMSE: 4.418390944753962\n",
      "iteration group: 9\n",
      "RMSE: 4.41755906819601\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (500,9,18) and (162,) not aligned: 18 (dim 2) != 162 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-424-0e6ac741e9c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpred1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'res7'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-133-42848c8767f0>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(x, w, b)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (500,9,18) and (162,) not aligned: 18 (dim 2) != 162 (dim 0)"
     ]
    }
   ],
   "source": [
    "w1, rmse1 = train(train_x, train_y, 0.01, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration group: 0\n",
      "RMSE: 4.296846047673093\n",
      "iteration group: 1\n",
      "RMSE: 4.233331060789999\n",
      "iteration group: 2\n",
      "RMSE: 4.21901696765482\n",
      "iteration group: 3\n",
      "RMSE: 4.213951592407973\n",
      "iteration group: 4\n",
      "RMSE: 4.211379264608519\n",
      "iteration group: 5\n",
      "RMSE: 4.209676208002888\n",
      "iteration group: 6\n",
      "RMSE: 4.208358086481296\n",
      "iteration group: 7\n",
      "RMSE: 4.207250769446524\n",
      "iteration group: 8\n",
      "RMSE: 4.206279532045433\n",
      "iteration group: 9\n",
      "RMSE: 4.205406642655287\n"
     ]
    }
   ],
   "source": [
    "w6, rmse6 = train(train_x, train_y, 0.01, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration group: 0\n",
      "RMSE: 4.466130023977407\n",
      "iteration group: 1\n",
      "RMSE: 4.435451892579774\n",
      "iteration group: 2\n",
      "RMSE: 4.42835320393328\n",
      "iteration group: 3\n",
      "RMSE: 4.425085117548755\n",
      "iteration group: 4\n",
      "RMSE: 4.423017318240971\n",
      "iteration group: 5\n",
      "RMSE: 4.421489766774898\n",
      "iteration group: 6\n",
      "RMSE: 4.420256740626775\n",
      "iteration group: 7\n",
      "RMSE: 4.419205431115259\n",
      "iteration group: 8\n",
      "RMSE: 4.418277397035291\n",
      "iteration group: 9\n",
      "RMSE: 4.417439526799195\n"
     ]
    }
   ],
   "source": [
    "w7, rmse7 = train(train_x1, train_y, 0.01, 10000)\n",
    "x_test2 = add_constant(x_test)\n",
    "pred7 = my_predict(x_test2, w7, 'res9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred7 = my_predict(x_test3, w7, 'res9')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (500,9,18) and (162,) not aligned: 18 (dim 2) != 162 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-428-c6e0d46590a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mpred1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'res7'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-428-c6e0d46590a7>\u001b[0m in \u001b[0;36mmy_predict\u001b[0;34m(test_x, ws, out)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmy_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mws\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sample_submission.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpred_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'value'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (500,9,18) and (162,) not aligned: 18 (dim 2) != 162 (dim 0)"
     ]
    }
   ],
   "source": [
    "def my_predict(test_x, ws, out):    \n",
    "    sample = pd.read_csv('sample_submission.csv')\n",
    "    pred_y = np.dot(test_x,ws)\n",
    "    for i, y in enumerate(pred_y):\n",
    "        sample.at[i,'value'] = y\n",
    "    sample.value = np.where(sample.value < 0, 0,sample.value)\n",
    "    sample.to_csv(out+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TA Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-374-7f34105e2081>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpred3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'res7'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-372-879c4ae9a2c9>\u001b[0m in \u001b[0;36mminibatch\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0mx_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;31m# 計算gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "w3, bias3 = minibatch(train_x, train_y)\n",
    "pred3 = predict(x_test, w3, bias3)\n",
    "result(pred3, 'res7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # 同學這邊要自己吃csv files\n",
    "    #uploaded = files.upload()\n",
    "    year1_pd = pd.read_csv('year1-data.csv')\n",
    "\n",
    "    year1 = readdata(year1_pd)\n",
    "    train_data = extract(year1)\n",
    "    train_x, train_y = parse2train(train_data)\n",
    "    \n",
    "    w, bias = minibatch(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse2test(data):\n",
    "    x = []\n",
    "    \n",
    "    # 用前面9筆資料預測下一筆PM2.5 所以需要-9\n",
    "    num_data = data.shape[1]//9\n",
    "    for i in range(num_data):\n",
    "        x_tmp = data[:,i*9:(i+1)*9]\n",
    "        x.append(x_tmp.reshape(-1,))\n",
    "    # x 會是一個(n, 18, 9)的陣列\n",
    "    x = np.array(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse2test2(data):\n",
    "    x = []\n",
    "    \n",
    "    # 用前面9筆資料預測下一筆PM2.5 所以需要-9\n",
    "    num_data = data.shape[1]//9\n",
    "    for i in range(num_data):\n",
    "        x_tmp = data[:,i*9:(i+1)*9]\n",
    "        pm25 = x_tmp[9,:]\n",
    "        for i, v in enumerate(pm25):\n",
    "            if v <= mean_test - 3.8*std_test or v >= mean_test + 3.8*std_test:\n",
    "                pm25[i] = mean_test\n",
    "        x_tmp[9,:] = pm25\n",
    "        x.append(x_tmp.reshape(-1,))\n",
    "    # x 會是一個(n, 18, 9)的陣列\n",
    "    x = np.array(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, w, b):\n",
    "    y = np.dot(x, w) + b\n",
    "    return(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result(y, file):\n",
    "    pred = y.reshape(-1,)\n",
    "    sample = pd.read_csv('sample_submission.csv')\n",
    "    sample['value'] = y\n",
    "    sample.to_csv(file+'.csv', index=False)\n",
    "result(test_y, 'res1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pd1 = readdata(test_pd)\n",
    "test_pd2 = extract(test_pd1)\n",
    "x_test = parse2test(test_pd2)\n",
    "test_y = predict(x_test, w2, bias2)\n",
    "result(test_y, 'res2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pd1 = readdata(test_pd)\n",
    "test_pd2 = extract(test_pd1)\n",
    "x_test = parse2test2(test_pd2)\n",
    "x_test3 = add_constant(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 163)"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, LSTM, TimeDistributed, RepeatVector\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildTrain(train, pastDay=9, futureDay=1):\n",
    "    X_train, Y_train = [], []\n",
    "    for i in range(train.shape[0]-futureDay-pastDay):\n",
    "        X_train.append(np.array(train[i:i+pastDay]))\n",
    "        Y_train.append(np.array(train[i+pastDay:i+pastDay+futureDay,9]))\n",
    "    return np.array(X_train), np.array(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(X,Y):\n",
    "    np.random.seed(10)\n",
    "    randomList = np.arange(X.shape[0])\n",
    "    np.random.shuffle(randomList)\n",
    "    return X[randomList], Y[randomList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(X,Y,rate):\n",
    "    X_train = X[int(X.shape[0]*rate):]\n",
    "    Y_train = Y[int(Y.shape[0]*rate):]\n",
    "    X_val = X[:int(X.shape[0]*rate)]\n",
    "    Y_val = Y[:int(Y.shape[0]*rate)]\n",
    "    return X_train, Y_train, X_val, Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildManyToOneModel(shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(9, input_length=shape[1], input_dim=shape[2]))\n",
    "    # output shape: (1, 1)\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "year1_pd = pd.read_csv('year1-data.csv')\n",
    "year2_pd = pd.read_csv('year2-data.csv')\n",
    "\n",
    "year1 = readdata(year1_pd)\n",
    "year2 = readdata(year2_pd)\n",
    "\n",
    "train_data1 = extract(year1)\n",
    "train_data2 = extract(year2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_rnn_train(train_data):\n",
    "    train_t = train_data.T\n",
    "    X_train, Y_train = buildTrain(train_t2, 9, 1)\n",
    "    X_train, Y_train = shuffle(X_train, Y_train)\n",
    "    X_train, Y_train, X_val, Y_val = splitData(X_train, Y_train, 0.15)\n",
    "    return(X_train, Y_train, X_val, Y_val)\n",
    "X_train1, Y_train1, X_val1, Y_val1 = to_rnn_train(train_data1)\n",
    "X_train2, Y_train2, X_val2, Y_val2 = to_rnn_train(train_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntuimb05/.pyenv/versions/3.7.3/envs/ccc/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/ntuimb05/.pyenv/versions/3.7.3/envs/ccc/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(9, input_shape=(9, 18))`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "W1007 09:13:28.168885 139719865399040 deprecation.py:323] From /home/ntuimb05/.pyenv/versions/3.7.3/envs/ccc/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_10 (LSTM)               (None, 9)                 1008      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 10        \n",
      "=================================================================\n",
      "Total params: 1,018\n",
      "Trainable params: 1,018\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1007 09:13:28.496730 139719865399040 deprecation_wrapper.py:119] From /home/ntuimb05/.pyenv/versions/3.7.3/envs/ccc/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "W1007 09:13:28.550365 139719865399040 deprecation_wrapper.py:119] From /home/ntuimb05/.pyenv/versions/3.7.3/envs/ccc/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7458 samples, validate on 1316 samples\n",
      "Epoch 1/1000\n",
      "7458/7458 [==============================] - 5s 623us/step - loss: 2097.9169 - val_loss: 4412.3625\n",
      "Epoch 2/1000\n",
      "7458/7458 [==============================] - 1s 123us/step - loss: 2048.1227 - val_loss: 4361.7415\n",
      "Epoch 3/1000\n",
      "7458/7458 [==============================] - 1s 122us/step - loss: 2009.6675 - val_loss: 4334.5652\n",
      "Epoch 4/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1984.6046 - val_loss: 4314.7601\n",
      "Epoch 5/1000\n",
      "7458/7458 [==============================] - 1s 129us/step - loss: 1965.3549 - val_loss: 4292.0720\n",
      "Epoch 6/1000\n",
      "7458/7458 [==============================] - 1s 120us/step - loss: 1951.0902 - val_loss: 4277.6854\n",
      "Epoch 7/1000\n",
      "7458/7458 [==============================] - 1s 123us/step - loss: 1938.9710 - val_loss: 4264.9897\n",
      "Epoch 8/1000\n",
      "7458/7458 [==============================] - 1s 122us/step - loss: 1927.4391 - val_loss: 4253.0012\n",
      "Epoch 9/1000\n",
      "7458/7458 [==============================] - 1s 130us/step - loss: 1917.4020 - val_loss: 4242.0156\n",
      "Epoch 10/1000\n",
      "7458/7458 [==============================] - 1s 128us/step - loss: 1907.7225 - val_loss: 4231.4477\n",
      "Epoch 11/1000\n",
      "7458/7458 [==============================] - 1s 130us/step - loss: 1899.0257 - val_loss: 4221.5268\n",
      "Epoch 12/1000\n",
      "7458/7458 [==============================] - 1s 122us/step - loss: 1890.5219 - val_loss: 4211.8918\n",
      "Epoch 13/1000\n",
      "7458/7458 [==============================] - 1s 122us/step - loss: 1882.5418 - val_loss: 4203.3076\n",
      "Epoch 14/1000\n",
      "7458/7458 [==============================] - 1s 124us/step - loss: 1875.1616 - val_loss: 4194.7921\n",
      "Epoch 15/1000\n",
      "7458/7458 [==============================] - 1s 123us/step - loss: 1868.3287 - val_loss: 4187.3530\n",
      "Epoch 16/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1861.4614 - val_loss: 4179.0468\n",
      "Epoch 17/1000\n",
      "7458/7458 [==============================] - 1s 129us/step - loss: 1854.6476 - val_loss: 4171.9219\n",
      "Epoch 18/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1847.6799 - val_loss: 4164.6914\n",
      "Epoch 19/1000\n",
      "7458/7458 [==============================] - 1s 126us/step - loss: 1841.4176 - val_loss: 4156.3431\n",
      "Epoch 20/1000\n",
      "7458/7458 [==============================] - 1s 123us/step - loss: 1836.2130 - val_loss: 4150.0987\n",
      "Epoch 21/1000\n",
      "7458/7458 [==============================] - 1s 121us/step - loss: 1832.0785 - val_loss: 4143.9066\n",
      "Epoch 22/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1826.0482 - val_loss: 4139.4849\n",
      "Epoch 23/1000\n",
      "7458/7458 [==============================] - 1s 127us/step - loss: 1820.8834 - val_loss: 4132.8200\n",
      "Epoch 24/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1816.1486 - val_loss: 4126.1249\n",
      "Epoch 25/1000\n",
      "7458/7458 [==============================] - 1s 129us/step - loss: 1812.2377 - val_loss: 4122.7439\n",
      "Epoch 26/1000\n",
      "7458/7458 [==============================] - 1s 122us/step - loss: 1808.7555 - val_loss: 4116.3985\n",
      "Epoch 27/1000\n",
      "7458/7458 [==============================] - 1s 127us/step - loss: 1805.3417 - val_loss: 4114.5677\n",
      "Epoch 28/1000\n",
      "7458/7458 [==============================] - 1s 127us/step - loss: 1801.4718 - val_loss: 4109.3983\n",
      "Epoch 29/1000\n",
      "7458/7458 [==============================] - 1s 133us/step - loss: 1796.8754 - val_loss: 4102.3796\n",
      "Epoch 30/1000\n",
      "7458/7458 [==============================] - 1s 124us/step - loss: 1794.6715 - val_loss: 4099.3342\n",
      "Epoch 31/1000\n",
      "7458/7458 [==============================] - 1s 123us/step - loss: 1789.7538 - val_loss: 4095.6569\n",
      "Epoch 32/1000\n",
      "7458/7458 [==============================] - 1s 128us/step - loss: 1786.4954 - val_loss: 4090.7755\n",
      "Epoch 33/1000\n",
      "7458/7458 [==============================] - 1s 124us/step - loss: 1784.4517 - val_loss: 4087.9211\n",
      "Epoch 34/1000\n",
      "7458/7458 [==============================] - 1s 128us/step - loss: 1779.9978 - val_loss: 4082.1310\n",
      "Epoch 35/1000\n",
      "7458/7458 [==============================] - 1s 122us/step - loss: 1777.2897 - val_loss: 4089.0147\n",
      "Epoch 36/1000\n",
      "7458/7458 [==============================] - 1s 123us/step - loss: 1775.0875 - val_loss: 4087.0564\n",
      "Epoch 37/1000\n",
      "7458/7458 [==============================] - 1s 126us/step - loss: 1772.9842 - val_loss: 4082.0327\n",
      "Epoch 38/1000\n",
      "7458/7458 [==============================] - 1s 128us/step - loss: 1769.6342 - val_loss: 4077.4246\n",
      "Epoch 39/1000\n",
      "7458/7458 [==============================] - 1s 123us/step - loss: 1766.6700 - val_loss: 4075.0182\n",
      "Epoch 40/1000\n",
      "7458/7458 [==============================] - 1s 124us/step - loss: 1764.2155 - val_loss: 4062.5619\n",
      "Epoch 41/1000\n",
      "7458/7458 [==============================] - 1s 127us/step - loss: 1761.8350 - val_loss: 4059.2501\n",
      "Epoch 42/1000\n",
      "7458/7458 [==============================] - 1s 127us/step - loss: 1757.5336 - val_loss: 4052.3126\n",
      "Epoch 43/1000\n",
      "7458/7458 [==============================] - 1s 128us/step - loss: 1756.6042 - val_loss: 4048.5396\n",
      "Epoch 44/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1753.1482 - val_loss: 4042.1536\n",
      "Epoch 45/1000\n",
      "7458/7458 [==============================] - 1s 127us/step - loss: 1749.4404 - val_loss: 4038.9960\n",
      "Epoch 46/1000\n",
      "7458/7458 [==============================] - 1s 121us/step - loss: 1743.6530 - val_loss: 4033.6696\n",
      "Epoch 47/1000\n",
      "7458/7458 [==============================] - 1s 123us/step - loss: 1740.8622 - val_loss: 4034.1667\n",
      "Epoch 48/1000\n",
      "7458/7458 [==============================] - 1s 124us/step - loss: 1733.0280 - val_loss: 3983.5380\n",
      "Epoch 49/1000\n",
      "7458/7458 [==============================] - 1s 128us/step - loss: 1727.4852 - val_loss: 3979.6009\n",
      "Epoch 50/1000\n",
      "7458/7458 [==============================] - 1s 123us/step - loss: 1724.8654 - val_loss: 3978.7140\n",
      "Epoch 51/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1723.3963 - val_loss: 3975.1345\n",
      "Epoch 52/1000\n",
      "7458/7458 [==============================] - 1s 128us/step - loss: 1719.8651 - val_loss: 3972.9591\n",
      "Epoch 53/1000\n",
      "7458/7458 [==============================] - 1s 124us/step - loss: 1717.5830 - val_loss: 3970.0569\n",
      "Epoch 54/1000\n",
      "7458/7458 [==============================] - 1s 122us/step - loss: 1715.3235 - val_loss: 3962.6839\n",
      "Epoch 55/1000\n",
      "7458/7458 [==============================] - 1s 127us/step - loss: 1712.3703 - val_loss: 3956.9561\n",
      "Epoch 56/1000\n",
      "7458/7458 [==============================] - 1s 128us/step - loss: 1712.2818 - val_loss: 3958.5171\n",
      "Epoch 57/1000\n",
      "7458/7458 [==============================] - 1s 127us/step - loss: 1709.4789 - val_loss: 3951.3665\n",
      "Epoch 58/1000\n",
      "7458/7458 [==============================] - 1s 126us/step - loss: 1706.4256 - val_loss: 3951.9701\n",
      "Epoch 59/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1704.3657 - val_loss: 3948.0278\n",
      "Epoch 60/1000\n",
      "7458/7458 [==============================] - 1s 124us/step - loss: 1701.7845 - val_loss: 3944.9815\n",
      "Epoch 61/1000\n",
      "7458/7458 [==============================] - 1s 126us/step - loss: 1699.7286 - val_loss: 3942.1056\n",
      "Epoch 62/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1698.5996 - val_loss: 3938.3405\n",
      "Epoch 63/1000\n",
      "7458/7458 [==============================] - 1s 122us/step - loss: 1696.4759 - val_loss: 3936.8875\n",
      "Epoch 64/1000\n",
      "7458/7458 [==============================] - 1s 127us/step - loss: 1694.2367 - val_loss: 3932.7198\n",
      "Epoch 65/1000\n",
      "7458/7458 [==============================] - 1s 122us/step - loss: 1692.2420 - val_loss: 3929.2606\n",
      "Epoch 66/1000\n",
      "7458/7458 [==============================] - 1s 128us/step - loss: 1691.0257 - val_loss: 3926.5677\n",
      "Epoch 67/1000\n",
      "7458/7458 [==============================] - 1s 123us/step - loss: 1690.3268 - val_loss: 3928.4920\n",
      "Epoch 68/1000\n",
      "7458/7458 [==============================] - 1s 128us/step - loss: 1688.4251 - val_loss: 3921.3461\n",
      "Epoch 69/1000\n",
      "7458/7458 [==============================] - 1s 133us/step - loss: 1684.9342 - val_loss: 3918.9335\n",
      "Epoch 70/1000\n",
      "7458/7458 [==============================] - 1s 131us/step - loss: 1683.9918 - val_loss: 3913.1553\n",
      "Epoch 71/1000\n",
      "7458/7458 [==============================] - 1s 124us/step - loss: 1681.5792 - val_loss: 3911.8968\n",
      "Epoch 72/1000\n",
      "7458/7458 [==============================] - 1s 126us/step - loss: 1679.2096 - val_loss: 3906.3306\n",
      "Epoch 73/1000\n",
      "7458/7458 [==============================] - 1s 126us/step - loss: 1679.9436 - val_loss: 3907.9551\n",
      "Epoch 74/1000\n",
      "7458/7458 [==============================] - 1s 126us/step - loss: 1676.8294 - val_loss: 3903.1945\n",
      "Epoch 75/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1676.4043 - val_loss: 3902.2832\n",
      "Epoch 76/1000\n",
      "7458/7458 [==============================] - 1s 127us/step - loss: 1674.0455 - val_loss: 3899.5148\n",
      "Epoch 77/1000\n",
      "7458/7458 [==============================] - 1s 129us/step - loss: 1672.3148 - val_loss: 3895.8043\n",
      "Epoch 78/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1671.8643 - val_loss: 3895.7583\n",
      "Epoch 79/1000\n",
      "7458/7458 [==============================] - 1s 127us/step - loss: 1671.8488 - val_loss: 3892.7334\n",
      "Epoch 80/1000\n",
      "7458/7458 [==============================] - 1s 126us/step - loss: 1668.5811 - val_loss: 3889.4055\n",
      "Epoch 81/1000\n",
      "7458/7458 [==============================] - 1s 121us/step - loss: 1670.3337 - val_loss: 3887.7771\n",
      "Epoch 82/1000\n",
      "7458/7458 [==============================] - 1s 123us/step - loss: 1666.5951 - val_loss: 3886.4988\n",
      "Epoch 83/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1665.6096 - val_loss: 3883.5447\n",
      "Epoch 84/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1662.1086 - val_loss: 3879.1176\n",
      "Epoch 85/1000\n",
      "7458/7458 [==============================] - 1s 126us/step - loss: 1661.8112 - val_loss: 3877.7933\n",
      "Epoch 86/1000\n",
      "7458/7458 [==============================] - 1s 126us/step - loss: 1660.1469 - val_loss: 3875.3272\n",
      "Epoch 87/1000\n",
      "7458/7458 [==============================] - 1s 121us/step - loss: 1658.2227 - val_loss: 3871.9143\n",
      "Epoch 88/1000\n",
      "7458/7458 [==============================] - 1s 122us/step - loss: 1657.3139 - val_loss: 3873.0543\n",
      "Epoch 89/1000\n",
      "7458/7458 [==============================] - 1s 121us/step - loss: 1657.2746 - val_loss: 3868.2677\n",
      "Epoch 90/1000\n",
      "7458/7458 [==============================] - 1s 129us/step - loss: 1654.4039 - val_loss: 3868.4878\n",
      "Epoch 91/1000\n",
      "7458/7458 [==============================] - 1s 124us/step - loss: 1654.0960 - val_loss: 3862.5691\n",
      "Epoch 92/1000\n",
      "7458/7458 [==============================] - 1s 130us/step - loss: 1653.9016 - val_loss: 3864.0853\n",
      "Epoch 93/1000\n",
      "7458/7458 [==============================] - 1s 120us/step - loss: 1652.1862 - val_loss: 3860.4362\n",
      "Epoch 94/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1650.5628 - val_loss: 3857.0553\n",
      "Epoch 95/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1648.9959 - val_loss: 3854.0923\n",
      "Epoch 96/1000\n",
      "7458/7458 [==============================] - 1s 124us/step - loss: 1647.7675 - val_loss: 3851.6817\n",
      "Epoch 97/1000\n",
      "7458/7458 [==============================] - 1s 127us/step - loss: 1646.7689 - val_loss: 3851.1185\n",
      "Epoch 98/1000\n",
      "7458/7458 [==============================] - 1s 127us/step - loss: 1647.5429 - val_loss: 3849.3576\n",
      "Epoch 99/1000\n",
      "7458/7458 [==============================] - 1s 127us/step - loss: 1644.5947 - val_loss: 3844.4099\n",
      "Epoch 100/1000\n",
      "7458/7458 [==============================] - 1s 128us/step - loss: 1643.7383 - val_loss: 3844.8525\n",
      "Epoch 101/1000\n",
      "7458/7458 [==============================] - 1s 130us/step - loss: 1643.2783 - val_loss: 3845.3900\n",
      "Epoch 102/1000\n",
      "7458/7458 [==============================] - 1s 122us/step - loss: 1641.8875 - val_loss: 3840.0280\n",
      "Epoch 103/1000\n",
      "7458/7458 [==============================] - 1s 128us/step - loss: 1640.9673 - val_loss: 3840.2312\n",
      "Epoch 104/1000\n",
      "7458/7458 [==============================] - 1s 124us/step - loss: 1640.3331 - val_loss: 3836.6036\n",
      "Epoch 105/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1639.9160 - val_loss: 3837.9149\n",
      "Epoch 106/1000\n",
      "7458/7458 [==============================] - 1s 124us/step - loss: 1640.2095 - val_loss: 3833.7356\n",
      "Epoch 107/1000\n",
      "7458/7458 [==============================] - 1s 127us/step - loss: 1637.1370 - val_loss: 3831.6653\n",
      "Epoch 108/1000\n",
      "7458/7458 [==============================] - 1s 127us/step - loss: 1637.7107 - val_loss: 3829.8961\n",
      "Epoch 109/1000\n",
      "7458/7458 [==============================] - 1s 126us/step - loss: 1635.2418 - val_loss: 3827.3523\n",
      "Epoch 110/1000\n",
      "7458/7458 [==============================] - 1s 122us/step - loss: 1634.1544 - val_loss: 3824.9686\n",
      "Epoch 111/1000\n",
      "7458/7458 [==============================] - 1s 127us/step - loss: 1633.7588 - val_loss: 3826.1785\n",
      "Epoch 112/1000\n",
      "7458/7458 [==============================] - 1s 126us/step - loss: 1632.0980 - val_loss: 3821.9704\n",
      "Epoch 113/1000\n",
      "7458/7458 [==============================] - 1s 126us/step - loss: 1631.5203 - val_loss: 3819.6000\n",
      "Epoch 114/1000\n",
      "7458/7458 [==============================] - 1s 124us/step - loss: 1631.6311 - val_loss: 3818.5107\n",
      "Epoch 115/1000\n",
      "7458/7458 [==============================] - 1s 128us/step - loss: 1629.5272 - val_loss: 3817.8104\n",
      "Epoch 116/1000\n",
      "7458/7458 [==============================] - 1s 126us/step - loss: 1629.2684 - val_loss: 3815.2705\n",
      "Epoch 117/1000\n",
      "7458/7458 [==============================] - 1s 127us/step - loss: 1629.9666 - val_loss: 3815.3992\n",
      "Epoch 118/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1628.1342 - val_loss: 3813.2374\n",
      "Epoch 119/1000\n",
      "7458/7458 [==============================] - 1s 127us/step - loss: 1628.4855 - val_loss: 3813.4741\n",
      "Epoch 120/1000\n",
      "7458/7458 [==============================] - 1s 122us/step - loss: 1627.1060 - val_loss: 3809.6556\n",
      "Epoch 121/1000\n",
      "7458/7458 [==============================] - 1s 128us/step - loss: 1625.1557 - val_loss: 3809.8308\n",
      "Epoch 122/1000\n",
      "7458/7458 [==============================] - 1s 123us/step - loss: 1625.7248 - val_loss: 3805.9761\n",
      "Epoch 123/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1623.9574 - val_loss: 3804.4254\n",
      "Epoch 124/1000\n",
      "7458/7458 [==============================] - 1s 126us/step - loss: 1623.3612 - val_loss: 3803.6982\n",
      "Epoch 125/1000\n",
      "7458/7458 [==============================] - 1s 123us/step - loss: 1623.1518 - val_loss: 3802.1876\n",
      "Epoch 126/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1622.0869 - val_loss: 3800.8342\n",
      "Epoch 127/1000\n",
      "7458/7458 [==============================] - 1s 127us/step - loss: 1621.1798 - val_loss: 3800.0053\n",
      "Epoch 128/1000\n",
      "7458/7458 [==============================] - 1s 126us/step - loss: 1619.3897 - val_loss: 3796.0608\n",
      "Epoch 129/1000\n",
      "7458/7458 [==============================] - 1s 122us/step - loss: 1620.5794 - val_loss: 3800.1486\n",
      "Epoch 130/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1620.3554 - val_loss: 3793.6405\n",
      "Epoch 131/1000\n",
      "7458/7458 [==============================] - 1s 123us/step - loss: 1618.1964 - val_loss: 3792.0695\n",
      "Epoch 132/1000\n",
      "7458/7458 [==============================] - 1s 132us/step - loss: 1618.9515 - val_loss: 3792.5577\n",
      "Epoch 133/1000\n",
      "7458/7458 [==============================] - 1s 126us/step - loss: 1617.4129 - val_loss: 3791.3748\n",
      "Epoch 134/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1616.4177 - val_loss: 3789.6694\n",
      "Epoch 135/1000\n",
      "7458/7458 [==============================] - 1s 129us/step - loss: 1615.4245 - val_loss: 3787.3767\n",
      "Epoch 136/1000\n",
      "7458/7458 [==============================] - 1s 127us/step - loss: 1616.4062 - val_loss: 3785.5275\n",
      "Epoch 137/1000\n",
      "7458/7458 [==============================] - 1s 121us/step - loss: 1615.1246 - val_loss: 3784.4778\n",
      "Epoch 138/1000\n",
      "7458/7458 [==============================] - 1s 128us/step - loss: 1614.9528 - val_loss: 3785.8312\n",
      "Epoch 139/1000\n",
      "7458/7458 [==============================] - 1s 127us/step - loss: 1614.2703 - val_loss: 3783.1859\n",
      "Epoch 140/1000\n",
      "7458/7458 [==============================] - 1s 124us/step - loss: 1612.7368 - val_loss: 3782.9601\n",
      "Epoch 141/1000\n",
      "7458/7458 [==============================] - 1s 122us/step - loss: 1611.9455 - val_loss: 3779.7787\n",
      "Epoch 142/1000\n",
      "7458/7458 [==============================] - 1s 120us/step - loss: 1611.7398 - val_loss: 3779.3676\n",
      "Epoch 143/1000\n",
      "7458/7458 [==============================] - 1s 127us/step - loss: 1610.9391 - val_loss: 3776.6009\n",
      "Epoch 144/1000\n",
      "7458/7458 [==============================] - 1s 120us/step - loss: 1609.1284 - val_loss: 3775.8023\n",
      "Epoch 145/1000\n",
      "7458/7458 [==============================] - 1s 127us/step - loss: 1610.0728 - val_loss: 3774.1161\n",
      "Epoch 146/1000\n",
      "7458/7458 [==============================] - 1s 129us/step - loss: 1609.4553 - val_loss: 3773.8117\n",
      "Epoch 147/1000\n",
      "7458/7458 [==============================] - 1s 122us/step - loss: 1607.8037 - val_loss: 3773.4324\n",
      "Epoch 148/1000\n",
      "7458/7458 [==============================] - 1s 123us/step - loss: 1614.2014 - val_loss: 3777.4272\n",
      "Epoch 149/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1609.5311 - val_loss: 3770.9862\n",
      "Epoch 150/1000\n",
      "7458/7458 [==============================] - 1s 124us/step - loss: 1606.3216 - val_loss: 3768.6410\n",
      "Epoch 151/1000\n",
      "7458/7458 [==============================] - 1s 128us/step - loss: 1606.6732 - val_loss: 3767.0317\n",
      "Epoch 152/1000\n",
      "7458/7458 [==============================] - 1s 128us/step - loss: 1606.1925 - val_loss: 3767.1142\n",
      "Epoch 153/1000\n",
      "7458/7458 [==============================] - 1s 128us/step - loss: 1606.0047 - val_loss: 3765.4416\n",
      "Epoch 154/1000\n",
      "7458/7458 [==============================] - 1s 127us/step - loss: 1604.9384 - val_loss: 3765.7752\n",
      "Epoch 155/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1603.8799 - val_loss: 3762.1770\n",
      "Epoch 156/1000\n",
      "7458/7458 [==============================] - 1s 122us/step - loss: 1604.3107 - val_loss: 3762.7015\n",
      "Epoch 157/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1603.2600 - val_loss: 3760.4057\n",
      "Epoch 158/1000\n",
      "7458/7458 [==============================] - 1s 124us/step - loss: 1602.4222 - val_loss: 3759.2546\n",
      "Epoch 159/1000\n",
      "7458/7458 [==============================] - 1s 128us/step - loss: 1602.2129 - val_loss: 3758.1100\n",
      "Epoch 160/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1601.2662 - val_loss: 3757.8011\n",
      "Epoch 161/1000\n",
      "7458/7458 [==============================] - 1s 129us/step - loss: 1601.1079 - val_loss: 3755.1103\n",
      "Epoch 162/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1599.6483 - val_loss: 3754.2705\n",
      "Epoch 163/1000\n",
      "7458/7458 [==============================] - 1s 124us/step - loss: 1599.4679 - val_loss: 3753.1009\n",
      "Epoch 164/1000\n",
      "7458/7458 [==============================] - 1s 123us/step - loss: 1599.2784 - val_loss: 3752.6614\n",
      "Epoch 165/1000\n",
      "7458/7458 [==============================] - 1s 126us/step - loss: 1598.5866 - val_loss: 3750.0145\n",
      "Epoch 166/1000\n",
      "7458/7458 [==============================] - 1s 127us/step - loss: 1599.0540 - val_loss: 3750.8929\n",
      "Epoch 167/1000\n",
      "7458/7458 [==============================] - 1s 127us/step - loss: 1598.4253 - val_loss: 3749.6254\n",
      "Epoch 168/1000\n",
      "7458/7458 [==============================] - 1s 130us/step - loss: 1597.8458 - val_loss: 3748.7834\n",
      "Epoch 169/1000\n",
      "7458/7458 [==============================] - 1s 128us/step - loss: 1597.7613 - val_loss: 3746.9463\n",
      "Epoch 170/1000\n",
      "7458/7458 [==============================] - 1s 127us/step - loss: 1597.4663 - val_loss: 3745.8825\n",
      "Epoch 171/1000\n",
      "7458/7458 [==============================] - 1s 123us/step - loss: 1596.4384 - val_loss: 3745.1729\n",
      "Epoch 172/1000\n",
      "7458/7458 [==============================] - 1s 124us/step - loss: 1594.6570 - val_loss: 3742.9165\n",
      "Epoch 173/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1594.4555 - val_loss: 3741.1896\n",
      "Epoch 174/1000\n",
      "7458/7458 [==============================] - 1s 128us/step - loss: 1593.9891 - val_loss: 3741.4671\n",
      "Epoch 175/1000\n",
      "7458/7458 [==============================] - 1s 124us/step - loss: 1593.6853 - val_loss: 3739.3054\n",
      "Epoch 176/1000\n",
      "7458/7458 [==============================] - 1s 129us/step - loss: 1595.0023 - val_loss: 3738.3775\n",
      "Epoch 177/1000\n",
      "7458/7458 [==============================] - 1s 129us/step - loss: 1592.4291 - val_loss: 3737.5318\n",
      "Epoch 178/1000\n",
      "7458/7458 [==============================] - 1s 129us/step - loss: 1594.4320 - val_loss: 3738.3942\n",
      "Epoch 179/1000\n",
      "7458/7458 [==============================] - 1s 126us/step - loss: 1591.3115 - val_loss: 3734.7749\n",
      "Epoch 180/1000\n",
      "7458/7458 [==============================] - 1s 122us/step - loss: 1591.2817 - val_loss: 3733.1386\n",
      "Epoch 181/1000\n",
      "7458/7458 [==============================] - 1s 127us/step - loss: 1591.4076 - val_loss: 3732.7217\n",
      "Epoch 182/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1592.5641 - val_loss: 3732.1335\n",
      "Epoch 183/1000\n",
      "7458/7458 [==============================] - 1s 129us/step - loss: 1590.9879 - val_loss: 3730.9988\n",
      "Epoch 184/1000\n",
      "7458/7458 [==============================] - 1s 122us/step - loss: 1589.4884 - val_loss: 3729.4071\n",
      "Epoch 185/1000\n",
      "7458/7458 [==============================] - 1s 130us/step - loss: 1589.2973 - val_loss: 3728.6828\n",
      "Epoch 186/1000\n",
      "7458/7458 [==============================] - 1s 129us/step - loss: 1588.3565 - val_loss: 3726.8259\n",
      "Epoch 187/1000\n",
      "7458/7458 [==============================] - 1s 126us/step - loss: 1588.5411 - val_loss: 3729.1956\n",
      "Epoch 188/1000\n",
      "7458/7458 [==============================] - 1s 128us/step - loss: 1591.5941 - val_loss: 3726.5428\n",
      "Epoch 189/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1587.4676 - val_loss: 3724.2089\n",
      "Epoch 190/1000\n",
      "7458/7458 [==============================] - 1s 119us/step - loss: 1587.4080 - val_loss: 3725.0329\n",
      "Epoch 191/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1588.0130 - val_loss: 3722.1231\n",
      "Epoch 192/1000\n",
      "7458/7458 [==============================] - 1s 131us/step - loss: 1586.9707 - val_loss: 3722.1140\n",
      "Epoch 193/1000\n",
      "7458/7458 [==============================] - 1s 122us/step - loss: 1586.1029 - val_loss: 3721.5047\n",
      "Epoch 194/1000\n",
      "7458/7458 [==============================] - 1s 130us/step - loss: 1585.8456 - val_loss: 3718.0089\n",
      "Epoch 195/1000\n",
      "7458/7458 [==============================] - 1s 124us/step - loss: 1586.8752 - val_loss: 3716.5638\n",
      "Epoch 196/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1584.9572 - val_loss: 3718.8300\n",
      "Epoch 197/1000\n",
      "7458/7458 [==============================] - 1s 123us/step - loss: 1584.4203 - val_loss: 3716.4398\n",
      "Epoch 198/1000\n",
      "7458/7458 [==============================] - 1s 121us/step - loss: 1583.5137 - val_loss: 3715.0855\n",
      "Epoch 199/1000\n",
      "7458/7458 [==============================] - 1s 124us/step - loss: 1584.7395 - val_loss: 3716.0975\n",
      "Epoch 200/1000\n",
      "7458/7458 [==============================] - 1s 122us/step - loss: 1583.9210 - val_loss: 3713.8712\n",
      "Epoch 201/1000\n",
      "7458/7458 [==============================] - 1s 124us/step - loss: 1583.9270 - val_loss: 3712.2736\n",
      "Epoch 202/1000\n",
      "7458/7458 [==============================] - 1s 130us/step - loss: 1582.9646 - val_loss: 3711.4354\n",
      "Epoch 203/1000\n",
      "7458/7458 [==============================] - 1s 132us/step - loss: 1582.2105 - val_loss: 3710.9309\n",
      "Epoch 204/1000\n",
      "7458/7458 [==============================] - 1s 128us/step - loss: 1583.1680 - val_loss: 3711.8339\n",
      "Epoch 205/1000\n",
      "7458/7458 [==============================] - 1s 129us/step - loss: 1581.3723 - val_loss: 3708.5330\n",
      "Epoch 206/1000\n",
      "7458/7458 [==============================] - 1s 123us/step - loss: 1581.6832 - val_loss: 3707.8508\n",
      "Epoch 207/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1582.5524 - val_loss: 3706.5519\n",
      "Epoch 208/1000\n",
      "7458/7458 [==============================] - 1s 128us/step - loss: 1580.5162 - val_loss: 3705.1028\n",
      "Epoch 209/1000\n",
      "7458/7458 [==============================] - 1s 123us/step - loss: 1581.6897 - val_loss: 3704.1043\n",
      "Epoch 210/1000\n",
      "7458/7458 [==============================] - 1s 130us/step - loss: 1582.0725 - val_loss: 3703.4151\n",
      "Epoch 211/1000\n",
      "7458/7458 [==============================] - 1s 129us/step - loss: 1578.0791 - val_loss: 3702.5416\n",
      "Epoch 212/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1581.1558 - val_loss: 3702.1141\n",
      "Epoch 213/1000\n",
      "7458/7458 [==============================] - 1s 126us/step - loss: 1579.5016 - val_loss: 3704.9482\n",
      "Epoch 214/1000\n",
      "7458/7458 [==============================] - 1s 128us/step - loss: 1580.0752 - val_loss: 3700.2103\n",
      "Epoch 215/1000\n",
      "7458/7458 [==============================] - 1s 132us/step - loss: 1576.9367 - val_loss: 3699.2855\n",
      "Epoch 216/1000\n",
      "7458/7458 [==============================] - 1s 130us/step - loss: 1578.3515 - val_loss: 3697.5724\n",
      "Epoch 217/1000\n",
      "7458/7458 [==============================] - 1s 122us/step - loss: 1578.4970 - val_loss: 3697.3032\n",
      "Epoch 218/1000\n",
      "7458/7458 [==============================] - 1s 123us/step - loss: 1578.8652 - val_loss: 3696.4858\n",
      "Epoch 219/1000\n",
      "7458/7458 [==============================] - 1s 122us/step - loss: 1576.8544 - val_loss: 3694.8479\n",
      "Epoch 220/1000\n",
      "7458/7458 [==============================] - 1s 127us/step - loss: 1575.6103 - val_loss: 3695.1713\n",
      "Epoch 221/1000\n",
      "7458/7458 [==============================] - 1s 120us/step - loss: 1576.6038 - val_loss: 3694.8224\n",
      "Epoch 222/1000\n",
      "7458/7458 [==============================] - 1s 124us/step - loss: 1575.0354 - val_loss: 3693.9504\n",
      "Epoch 223/1000\n",
      "7458/7458 [==============================] - 1s 128us/step - loss: 1578.4175 - val_loss: 3696.1458\n",
      "Epoch 224/1000\n",
      "7458/7458 [==============================] - 1s 127us/step - loss: 1577.5441 - val_loss: 3691.3668\n",
      "Epoch 225/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1573.9555 - val_loss: 3690.3457\n",
      "Epoch 226/1000\n",
      "7458/7458 [==============================] - 1s 122us/step - loss: 1573.2437 - val_loss: 3690.6411\n",
      "Epoch 227/1000\n",
      "7458/7458 [==============================] - 1s 126us/step - loss: 1573.8253 - val_loss: 3690.0004\n",
      "Epoch 228/1000\n",
      "7458/7458 [==============================] - 1s 128us/step - loss: 1573.2162 - val_loss: 3688.3291\n",
      "Epoch 229/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1576.0548 - val_loss: 3687.0218\n",
      "Epoch 230/1000\n",
      "7458/7458 [==============================] - 1s 128us/step - loss: 1573.3748 - val_loss: 3687.1557\n",
      "Epoch 231/1000\n",
      "7458/7458 [==============================] - 1s 120us/step - loss: 1572.3204 - val_loss: 3686.2045\n",
      "Epoch 232/1000\n",
      "7458/7458 [==============================] - 1s 126us/step - loss: 1571.5227 - val_loss: 3684.4762\n",
      "Epoch 233/1000\n",
      "7458/7458 [==============================] - 1s 124us/step - loss: 1572.3096 - val_loss: 3684.4787\n",
      "Epoch 234/1000\n",
      "7458/7458 [==============================] - 1s 127us/step - loss: 1577.9692 - val_loss: 3686.4003\n",
      "Epoch 235/1000\n",
      "7458/7458 [==============================] - 1s 126us/step - loss: 1574.7146 - val_loss: 3681.8615\n",
      "Epoch 236/1000\n",
      "7458/7458 [==============================] - 1s 128us/step - loss: 1572.1262 - val_loss: 3681.0712\n",
      "Epoch 237/1000\n",
      "7458/7458 [==============================] - 1s 126us/step - loss: 1571.2113 - val_loss: 3679.7597\n",
      "Epoch 238/1000\n",
      "7458/7458 [==============================] - 1s 126us/step - loss: 1571.0339 - val_loss: 3680.8884\n",
      "Epoch 239/1000\n",
      "7458/7458 [==============================] - 1s 130us/step - loss: 1569.2814 - val_loss: 3678.7859\n",
      "Epoch 240/1000\n",
      "7458/7458 [==============================] - 1s 128us/step - loss: 1570.7867 - val_loss: 3680.2970\n",
      "Epoch 241/1000\n",
      "7458/7458 [==============================] - 1s 128us/step - loss: 1570.6679 - val_loss: 3677.8951\n",
      "Epoch 242/1000\n",
      "7458/7458 [==============================] - 1s 129us/step - loss: 1571.9703 - val_loss: 3679.4018\n",
      "Epoch 243/1000\n",
      "7458/7458 [==============================] - 1s 131us/step - loss: 1568.8975 - val_loss: 3675.0956\n",
      "Epoch 244/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1570.8741 - val_loss: 3675.9727\n",
      "Epoch 245/1000\n",
      "7458/7458 [==============================] - 1s 123us/step - loss: 1568.2459 - val_loss: 3675.9222\n",
      "Epoch 246/1000\n",
      "7458/7458 [==============================] - 1s 123us/step - loss: 1568.2194 - val_loss: 3674.7078\n",
      "Epoch 247/1000\n",
      "7458/7458 [==============================] - 1s 129us/step - loss: 1568.8477 - val_loss: 3674.7834\n",
      "Epoch 248/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1567.4974 - val_loss: 3671.8028\n",
      "Epoch 249/1000\n",
      "7458/7458 [==============================] - 1s 128us/step - loss: 1567.9259 - val_loss: 3670.1646\n",
      "Epoch 250/1000\n",
      "7458/7458 [==============================] - 1s 121us/step - loss: 1567.5922 - val_loss: 3669.8060\n",
      "Epoch 251/1000\n",
      "7458/7458 [==============================] - 1s 122us/step - loss: 1566.5306 - val_loss: 3671.0887\n",
      "Epoch 252/1000\n",
      "7458/7458 [==============================] - 1s 127us/step - loss: 1567.9285 - val_loss: 3670.2528\n",
      "Epoch 253/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1566.7491 - val_loss: 3666.7201\n",
      "Epoch 254/1000\n",
      "7458/7458 [==============================] - 1s 122us/step - loss: 1564.9109 - val_loss: 3666.1884\n",
      "Epoch 255/1000\n",
      "7458/7458 [==============================] - 1s 124us/step - loss: 1565.4181 - val_loss: 3666.2059\n",
      "Epoch 256/1000\n",
      "7458/7458 [==============================] - 1s 126us/step - loss: 1570.5569 - val_loss: 3669.3620\n",
      "Epoch 257/1000\n",
      "7458/7458 [==============================] - 1s 124us/step - loss: 1568.7854 - val_loss: 3668.4323\n",
      "Epoch 258/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1564.0018 - val_loss: 3663.0649\n",
      "Epoch 259/1000\n",
      "7458/7458 [==============================] - 1s 127us/step - loss: 1564.7256 - val_loss: 3661.3915\n",
      "Epoch 260/1000\n",
      "7458/7458 [==============================] - 1s 129us/step - loss: 1563.6074 - val_loss: 3664.4948\n",
      "Epoch 261/1000\n",
      "7458/7458 [==============================] - 1s 124us/step - loss: 1562.5017 - val_loss: 3661.2627\n",
      "Epoch 262/1000\n",
      "7458/7458 [==============================] - 1s 123us/step - loss: 1562.6879 - val_loss: 3661.4742\n",
      "Epoch 263/1000\n",
      "7458/7458 [==============================] - 1s 121us/step - loss: 1562.5449 - val_loss: 3662.3058\n",
      "Epoch 264/1000\n",
      "7458/7458 [==============================] - 1s 124us/step - loss: 1562.3406 - val_loss: 3660.2510\n",
      "Epoch 265/1000\n",
      "7458/7458 [==============================] - 1s 124us/step - loss: 1561.2578 - val_loss: 3657.1443\n",
      "Epoch 266/1000\n",
      "7458/7458 [==============================] - 1s 119us/step - loss: 1560.7752 - val_loss: 3657.4568\n",
      "Epoch 267/1000\n",
      "7458/7458 [==============================] - 1s 126us/step - loss: 1560.5996 - val_loss: 3656.0740\n",
      "Epoch 268/1000\n",
      "7458/7458 [==============================] - 1s 128us/step - loss: 1562.0088 - val_loss: 3655.3920\n",
      "Epoch 269/1000\n",
      "7458/7458 [==============================] - 1s 127us/step - loss: 1563.4786 - val_loss: 3655.3111\n",
      "Epoch 270/1000\n",
      "7458/7458 [==============================] - 1s 126us/step - loss: 1560.2351 - val_loss: 3653.9965\n",
      "Epoch 271/1000\n",
      "7458/7458 [==============================] - 1s 129us/step - loss: 1559.7187 - val_loss: 3653.0498\n",
      "Epoch 272/1000\n",
      "7458/7458 [==============================] - 1s 126us/step - loss: 1558.6985 - val_loss: 3652.8617\n",
      "Epoch 273/1000\n",
      "7458/7458 [==============================] - 1s 130us/step - loss: 1558.2301 - val_loss: 3651.1467\n",
      "Epoch 274/1000\n",
      "7458/7458 [==============================] - 1s 128us/step - loss: 1561.3295 - val_loss: 3651.0461\n",
      "Epoch 275/1000\n",
      "7458/7458 [==============================] - 1s 131us/step - loss: 1558.5124 - val_loss: 3649.5880\n",
      "Epoch 276/1000\n",
      "7458/7458 [==============================] - 1s 127us/step - loss: 1564.6650 - val_loss: 3655.6427\n",
      "Epoch 277/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1560.9005 - val_loss: 3650.1687\n",
      "Epoch 278/1000\n",
      "7458/7458 [==============================] - 1s 126us/step - loss: 1557.2390 - val_loss: 3647.0584\n",
      "Epoch 279/1000\n",
      "7458/7458 [==============================] - 1s 123us/step - loss: 1556.7223 - val_loss: 3648.4242\n",
      "Epoch 280/1000\n",
      "7458/7458 [==============================] - 1s 121us/step - loss: 1558.3129 - val_loss: 3647.3398\n",
      "Epoch 281/1000\n",
      "7458/7458 [==============================] - 1s 127us/step - loss: 1557.4702 - val_loss: 3645.3227\n",
      "Epoch 282/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1555.6946 - val_loss: 3644.5286\n",
      "Epoch 283/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1558.4740 - val_loss: 3650.1907\n",
      "Epoch 284/1000\n",
      "7458/7458 [==============================] - 1s 123us/step - loss: 1557.7859 - val_loss: 3641.8699\n",
      "Epoch 285/1000\n",
      "7458/7458 [==============================] - 1s 130us/step - loss: 1556.6667 - val_loss: 3642.9287\n",
      "Epoch 286/1000\n",
      "7458/7458 [==============================] - 1s 122us/step - loss: 1556.0718 - val_loss: 3642.8308\n",
      "Epoch 287/1000\n",
      "7458/7458 [==============================] - 1s 128us/step - loss: 1555.6019 - val_loss: 3641.6305\n",
      "Epoch 288/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1554.7765 - val_loss: 3641.6464\n",
      "Epoch 289/1000\n",
      "7458/7458 [==============================] - 1s 126us/step - loss: 1554.3020 - val_loss: 3639.8946\n",
      "Epoch 290/1000\n",
      "7458/7458 [==============================] - 1s 124us/step - loss: 1554.2555 - val_loss: 3639.7195\n",
      "Epoch 291/1000\n",
      "7458/7458 [==============================] - 1s 129us/step - loss: 1553.3718 - val_loss: 3637.5126\n",
      "Epoch 292/1000\n",
      "7458/7458 [==============================] - 1s 122us/step - loss: 1553.3296 - val_loss: 3637.5565\n",
      "Epoch 293/1000\n",
      "7458/7458 [==============================] - 1s 120us/step - loss: 1555.3373 - val_loss: 3642.3117\n",
      "Epoch 294/1000\n",
      "7458/7458 [==============================] - 1s 123us/step - loss: 1555.1518 - val_loss: 3635.4211\n",
      "Epoch 295/1000\n",
      "7458/7458 [==============================] - 1s 127us/step - loss: 1552.4809 - val_loss: 3633.4844\n",
      "Epoch 296/1000\n",
      "7458/7458 [==============================] - 1s 126us/step - loss: 1553.6463 - val_loss: 3633.4959\n",
      "Epoch 297/1000\n",
      "7458/7458 [==============================] - 1s 127us/step - loss: 1554.7562 - val_loss: 3634.7707\n",
      "Epoch 298/1000\n",
      "7458/7458 [==============================] - 1s 130us/step - loss: 1552.6255 - val_loss: 3632.0473\n",
      "Epoch 299/1000\n",
      "7458/7458 [==============================] - 1s 119us/step - loss: 1553.7628 - val_loss: 3631.7856\n",
      "Epoch 300/1000\n",
      "7458/7458 [==============================] - 1s 128us/step - loss: 1551.3782 - val_loss: 3631.4235\n",
      "Epoch 301/1000\n",
      "7458/7458 [==============================] - 1s 123us/step - loss: 1550.6984 - val_loss: 3628.6042\n",
      "Epoch 302/1000\n",
      "7458/7458 [==============================] - 1s 122us/step - loss: 1553.0475 - val_loss: 3629.4613\n",
      "Epoch 303/1000\n",
      "7458/7458 [==============================] - 1s 128us/step - loss: 1550.3176 - val_loss: 3630.1711\n",
      "Epoch 304/1000\n",
      "7458/7458 [==============================] - 1s 124us/step - loss: 1550.9923 - val_loss: 3629.3719\n",
      "Epoch 305/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1551.3069 - val_loss: 3630.3534\n",
      "Epoch 306/1000\n",
      "7458/7458 [==============================] - 1s 128us/step - loss: 1551.4898 - val_loss: 3625.7418\n",
      "Epoch 307/1000\n",
      "7458/7458 [==============================] - 1s 128us/step - loss: 1552.3384 - val_loss: 3624.2876\n",
      "Epoch 308/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1552.3195 - val_loss: 3627.4211\n",
      "Epoch 309/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1549.8517 - val_loss: 3623.9980\n",
      "Epoch 310/1000\n",
      "7458/7458 [==============================] - 1s 128us/step - loss: 1548.8394 - val_loss: 3625.7441\n",
      "Epoch 311/1000\n",
      "7458/7458 [==============================] - 1s 131us/step - loss: 1548.6996 - val_loss: 3621.9275\n",
      "Epoch 312/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1550.9704 - val_loss: 3623.5223\n",
      "Epoch 313/1000\n",
      "7458/7458 [==============================] - 1s 128us/step - loss: 1548.6080 - val_loss: 3621.1528\n",
      "Epoch 314/1000\n",
      "7458/7458 [==============================] - 1s 126us/step - loss: 1547.1381 - val_loss: 3621.2758\n",
      "Epoch 315/1000\n",
      "7458/7458 [==============================] - 1s 129us/step - loss: 1547.5093 - val_loss: 3620.3087\n",
      "Epoch 316/1000\n",
      "7458/7458 [==============================] - 1s 130us/step - loss: 1549.1207 - val_loss: 3619.8413\n",
      "Epoch 317/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1545.7175 - val_loss: 3616.4869\n",
      "Epoch 318/1000\n",
      "7458/7458 [==============================] - 1s 124us/step - loss: 1546.3812 - val_loss: 3617.8762\n",
      "Epoch 319/1000\n",
      "7458/7458 [==============================] - 1s 130us/step - loss: 1546.9610 - val_loss: 3617.8182\n",
      "Epoch 320/1000\n",
      "7458/7458 [==============================] - 1s 123us/step - loss: 1549.4268 - val_loss: 3619.0200\n",
      "Epoch 321/1000\n",
      "7458/7458 [==============================] - 1s 129us/step - loss: 1545.8575 - val_loss: 3613.7463\n",
      "Epoch 322/1000\n",
      "7458/7458 [==============================] - 1s 126us/step - loss: 1545.9826 - val_loss: 3613.5627\n",
      "Epoch 323/1000\n",
      "7458/7458 [==============================] - 1s 126us/step - loss: 1548.9066 - val_loss: 3615.5458\n",
      "Epoch 324/1000\n",
      "7458/7458 [==============================] - 1s 123us/step - loss: 1548.2050 - val_loss: 3611.2702\n",
      "Epoch 325/1000\n",
      "7458/7458 [==============================] - 1s 121us/step - loss: 1544.5535 - val_loss: 3611.8667\n",
      "Epoch 326/1000\n",
      "7458/7458 [==============================] - 1s 126us/step - loss: 1543.9491 - val_loss: 3609.2934\n",
      "Epoch 327/1000\n",
      "7458/7458 [==============================] - 1s 130us/step - loss: 1543.4683 - val_loss: 3609.0633\n",
      "Epoch 328/1000\n",
      "7458/7458 [==============================] - 1s 128us/step - loss: 1543.2043 - val_loss: 3607.9482\n",
      "Epoch 329/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1542.8859 - val_loss: 3606.6350\n",
      "Epoch 330/1000\n",
      "7458/7458 [==============================] - 1s 127us/step - loss: 1543.8878 - val_loss: 3607.8893\n",
      "Epoch 331/1000\n",
      "7458/7458 [==============================] - 1s 128us/step - loss: 1546.2956 - val_loss: 3608.5483\n",
      "Epoch 332/1000\n",
      "7458/7458 [==============================] - 1s 126us/step - loss: 1542.7593 - val_loss: 3605.8083\n",
      "Epoch 333/1000\n",
      "7458/7458 [==============================] - 1s 129us/step - loss: 1541.3662 - val_loss: 3603.7382\n",
      "Epoch 334/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1544.2717 - val_loss: 3605.4999\n",
      "Epoch 335/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1543.1375 - val_loss: 3601.4056\n",
      "Epoch 336/1000\n",
      "7458/7458 [==============================] - 1s 123us/step - loss: 1544.0894 - val_loss: 3604.6267\n",
      "Epoch 337/1000\n",
      "7458/7458 [==============================] - 1s 124us/step - loss: 1541.5520 - val_loss: 3601.4373\n",
      "Epoch 338/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1541.5333 - val_loss: 3602.9321\n",
      "Epoch 339/1000\n",
      "7458/7458 [==============================] - 1s 124us/step - loss: 1541.0266 - val_loss: 3600.7078\n",
      "Epoch 340/1000\n",
      "7458/7458 [==============================] - 1s 124us/step - loss: 1541.5736 - val_loss: 3603.1119\n",
      "Epoch 341/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1540.7331 - val_loss: 3599.4871\n",
      "Epoch 342/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1540.9019 - val_loss: 3602.3931\n",
      "Epoch 343/1000\n",
      "7458/7458 [==============================] - 1s 124us/step - loss: 1544.1908 - val_loss: 3600.4564\n",
      "Epoch 344/1000\n",
      "7458/7458 [==============================] - 1s 123us/step - loss: 1541.7794 - val_loss: 3595.7053\n",
      "Epoch 345/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1540.7685 - val_loss: 3601.9370\n",
      "Epoch 346/1000\n",
      "7458/7458 [==============================] - 1s 126us/step - loss: 1540.5227 - val_loss: 3596.6055\n",
      "Epoch 347/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1539.4436 - val_loss: 3594.7639\n",
      "Epoch 348/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1539.3926 - val_loss: 3592.2033\n",
      "Epoch 349/1000\n",
      "7458/7458 [==============================] - 1s 120us/step - loss: 1538.9023 - val_loss: 3592.5532\n",
      "Epoch 350/1000\n",
      "7458/7458 [==============================] - 1s 128us/step - loss: 1538.5479 - val_loss: 3593.3496\n",
      "Epoch 351/1000\n",
      "7458/7458 [==============================] - 1s 128us/step - loss: 1538.2799 - val_loss: 3591.5631\n",
      "Epoch 352/1000\n",
      "7458/7458 [==============================] - 1s 123us/step - loss: 1540.9278 - val_loss: 3594.6672\n",
      "Epoch 353/1000\n",
      "7458/7458 [==============================] - 1s 127us/step - loss: 1538.0096 - val_loss: 3592.3697\n",
      "Epoch 354/1000\n",
      "7458/7458 [==============================] - 1s 121us/step - loss: 1541.6561 - val_loss: 3596.6523\n",
      "Epoch 355/1000\n",
      "7458/7458 [==============================] - 1s 129us/step - loss: 1539.8958 - val_loss: 3589.3504\n",
      "Epoch 356/1000\n",
      "7458/7458 [==============================] - 1s 126us/step - loss: 1540.7308 - val_loss: 3591.0950\n",
      "Epoch 357/1000\n",
      "7458/7458 [==============================] - 1s 122us/step - loss: 1537.4651 - val_loss: 3587.6845\n",
      "Epoch 358/1000\n",
      "7458/7458 [==============================] - 1s 120us/step - loss: 1537.9391 - val_loss: 3590.8586\n",
      "Epoch 359/1000\n",
      "7458/7458 [==============================] - 1s 127us/step - loss: 1538.3902 - val_loss: 3590.5312\n",
      "Epoch 360/1000\n",
      "7458/7458 [==============================] - 1s 121us/step - loss: 1540.6750 - val_loss: 3588.0745\n",
      "Epoch 361/1000\n",
      "7458/7458 [==============================] - 1s 127us/step - loss: 1535.7353 - val_loss: 3586.2325\n",
      "Epoch 362/1000\n",
      "7458/7458 [==============================] - 1s 127us/step - loss: 1536.8572 - val_loss: 3585.7031\n",
      "Epoch 363/1000\n",
      "7458/7458 [==============================] - 1s 124us/step - loss: 1537.9140 - val_loss: 3583.5812\n",
      "Epoch 364/1000\n",
      "7458/7458 [==============================] - 1s 123us/step - loss: 1535.6871 - val_loss: 3583.8612\n",
      "Epoch 365/1000\n",
      "7458/7458 [==============================] - 1s 122us/step - loss: 1537.7576 - val_loss: 3585.6435\n",
      "Epoch 366/1000\n",
      "7458/7458 [==============================] - 1s 124us/step - loss: 1535.3343 - val_loss: 3580.8954\n",
      "Epoch 367/1000\n",
      "7458/7458 [==============================] - 1s 128us/step - loss: 1538.3061 - val_loss: 3587.7095\n",
      "Epoch 368/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1534.8353 - val_loss: 3578.0783\n",
      "Epoch 369/1000\n",
      "7458/7458 [==============================] - 1s 121us/step - loss: 1544.7647 - val_loss: 3596.2513\n",
      "Epoch 370/1000\n",
      "7458/7458 [==============================] - 1s 126us/step - loss: 1539.4968 - val_loss: 3579.8189\n",
      "Epoch 371/1000\n",
      "7458/7458 [==============================] - 1s 124us/step - loss: 1534.8923 - val_loss: 3580.7172\n",
      "Epoch 372/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1538.4229 - val_loss: 3580.9737\n",
      "Epoch 373/1000\n",
      "7458/7458 [==============================] - 1s 128us/step - loss: 1534.0886 - val_loss: 3576.8783\n",
      "Epoch 374/1000\n",
      "7458/7458 [==============================] - 1s 130us/step - loss: 1532.7930 - val_loss: 3579.5052\n",
      "Epoch 375/1000\n",
      "7458/7458 [==============================] - 1s 131us/step - loss: 1533.4658 - val_loss: 3575.3851\n",
      "Epoch 376/1000\n",
      "7458/7458 [==============================] - 1s 129us/step - loss: 1531.5100 - val_loss: 3573.2994\n",
      "Epoch 377/1000\n",
      "7458/7458 [==============================] - 1s 121us/step - loss: 1536.5701 - val_loss: 3577.2049\n",
      "Epoch 378/1000\n",
      "7458/7458 [==============================] - 1s 126us/step - loss: 1532.6501 - val_loss: 3576.0216\n",
      "Epoch 379/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1534.6760 - val_loss: 3572.7937\n",
      "Epoch 380/1000\n",
      "7458/7458 [==============================] - 1s 127us/step - loss: 1533.2259 - val_loss: 3572.1505\n",
      "Epoch 381/1000\n",
      "7458/7458 [==============================] - 1s 126us/step - loss: 1531.5780 - val_loss: 3571.6369\n",
      "Epoch 382/1000\n",
      "7458/7458 [==============================] - 1s 127us/step - loss: 1530.4703 - val_loss: 3569.9394\n",
      "Epoch 383/1000\n",
      "7458/7458 [==============================] - 1s 126us/step - loss: 1531.6184 - val_loss: 3571.9047\n",
      "Epoch 384/1000\n",
      "7458/7458 [==============================] - 1s 126us/step - loss: 1531.5194 - val_loss: 3568.5928\n",
      "Epoch 385/1000\n",
      "7458/7458 [==============================] - 1s 126us/step - loss: 1532.6005 - val_loss: 3571.4624\n",
      "Epoch 386/1000\n",
      "7458/7458 [==============================] - 1s 123us/step - loss: 1535.3769 - val_loss: 3575.2044\n",
      "Epoch 387/1000\n",
      "7458/7458 [==============================] - 1s 132us/step - loss: 1537.6805 - val_loss: 3569.7505\n",
      "Epoch 388/1000\n",
      "7458/7458 [==============================] - 1s 128us/step - loss: 1530.4912 - val_loss: 3564.5860\n",
      "Epoch 389/1000\n",
      "7458/7458 [==============================] - 1s 123us/step - loss: 1530.9211 - val_loss: 3567.0969\n",
      "Epoch 390/1000\n",
      "7458/7458 [==============================] - 1s 128us/step - loss: 1529.9092 - val_loss: 3566.7337\n",
      "Epoch 391/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1529.6156 - val_loss: 3564.2173\n",
      "Epoch 392/1000\n",
      "7458/7458 [==============================] - 1s 127us/step - loss: 1529.9472 - val_loss: 3564.0651\n",
      "Epoch 393/1000\n",
      "7458/7458 [==============================] - 1s 124us/step - loss: 1529.7517 - val_loss: 3567.3080\n",
      "Epoch 394/1000\n",
      "7458/7458 [==============================] - 1s 124us/step - loss: 1533.9595 - val_loss: 3561.6914\n",
      "Epoch 395/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1530.5493 - val_loss: 3561.7598\n",
      "Epoch 396/1000\n",
      "7458/7458 [==============================] - 1s 123us/step - loss: 1528.2370 - val_loss: 3562.4974\n",
      "Epoch 397/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1532.0528 - val_loss: 3563.9625\n",
      "Epoch 398/1000\n",
      "7458/7458 [==============================] - 1s 129us/step - loss: 1527.5377 - val_loss: 3558.2987\n",
      "Epoch 399/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1534.1284 - val_loss: 3561.5385\n",
      "Epoch 400/1000\n",
      "7458/7458 [==============================] - 1s 123us/step - loss: 1528.1524 - val_loss: 3559.2398\n",
      "Epoch 401/1000\n",
      "7458/7458 [==============================] - 1s 123us/step - loss: 1527.0847 - val_loss: 3557.2047\n",
      "Epoch 402/1000\n",
      "7458/7458 [==============================] - 1s 123us/step - loss: 1534.2249 - val_loss: 3562.1935\n",
      "Epoch 403/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1527.4282 - val_loss: 3556.4646\n",
      "Epoch 404/1000\n",
      "7458/7458 [==============================] - 1s 124us/step - loss: 1526.7417 - val_loss: 3555.7520\n",
      "Epoch 405/1000\n",
      "7458/7458 [==============================] - 1s 131us/step - loss: 1528.9235 - val_loss: 3560.8897\n",
      "Epoch 406/1000\n",
      "7458/7458 [==============================] - 1s 130us/step - loss: 1532.5158 - val_loss: 3554.1887\n",
      "Epoch 407/1000\n",
      "7458/7458 [==============================] - 1s 127us/step - loss: 1525.2037 - val_loss: 3552.5132\n",
      "Epoch 408/1000\n",
      "7458/7458 [==============================] - 1s 123us/step - loss: 1527.4600 - val_loss: 3554.3415\n",
      "Epoch 409/1000\n",
      "7458/7458 [==============================] - 1s 127us/step - loss: 1529.2227 - val_loss: 3556.5362\n",
      "Epoch 410/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1529.7010 - val_loss: 3551.7978\n",
      "Epoch 411/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1524.6493 - val_loss: 3549.2995\n",
      "Epoch 412/1000\n",
      "7458/7458 [==============================] - 1s 129us/step - loss: 1525.1426 - val_loss: 3552.1332\n",
      "Epoch 413/1000\n",
      "7458/7458 [==============================] - 1s 124us/step - loss: 1524.3376 - val_loss: 3547.6432\n",
      "Epoch 414/1000\n",
      "7458/7458 [==============================] - 1s 123us/step - loss: 1526.7203 - val_loss: 3551.2335\n",
      "Epoch 415/1000\n",
      "7458/7458 [==============================] - 1s 124us/step - loss: 1524.5040 - val_loss: 3546.9193\n",
      "Epoch 416/1000\n",
      "7458/7458 [==============================] - 1s 124us/step - loss: 1524.4233 - val_loss: 3547.2799\n",
      "Epoch 417/1000\n",
      "7458/7458 [==============================] - 1s 123us/step - loss: 1525.2831 - val_loss: 3547.8113\n",
      "Epoch 418/1000\n",
      "7458/7458 [==============================] - 1s 124us/step - loss: 1521.7609 - val_loss: 3545.0894\n",
      "Epoch 419/1000\n",
      "7458/7458 [==============================] - 1s 128us/step - loss: 1521.3620 - val_loss: 3544.1956\n",
      "Epoch 420/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1522.2425 - val_loss: 3544.4444\n",
      "Epoch 421/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1523.0585 - val_loss: 3544.1456\n",
      "Epoch 422/1000\n",
      "7458/7458 [==============================] - 1s 124us/step - loss: 1520.4375 - val_loss: 3541.2741\n",
      "Epoch 423/1000\n",
      "7458/7458 [==============================] - 1s 129us/step - loss: 1521.7215 - val_loss: 3541.3610\n",
      "Epoch 424/1000\n",
      "7458/7458 [==============================] - 1s 126us/step - loss: 1520.1356 - val_loss: 3540.6152\n",
      "Epoch 425/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1518.3583 - val_loss: 3537.7811\n",
      "Epoch 426/1000\n",
      "7458/7458 [==============================] - 1s 128us/step - loss: 1520.1376 - val_loss: 3540.2214\n",
      "Epoch 427/1000\n",
      "7458/7458 [==============================] - 1s 126us/step - loss: 1519.1111 - val_loss: 3536.1283\n",
      "Epoch 428/1000\n",
      "7458/7458 [==============================] - 1s 121us/step - loss: 1518.6628 - val_loss: 3536.2885\n",
      "Epoch 429/1000\n",
      "7458/7458 [==============================] - 1s 124us/step - loss: 1518.5027 - val_loss: 3539.5101\n",
      "Epoch 430/1000\n",
      "7458/7458 [==============================] - 1s 127us/step - loss: 1520.1145 - val_loss: 3535.9419\n",
      "Epoch 431/1000\n",
      "7458/7458 [==============================] - 1s 123us/step - loss: 1516.8369 - val_loss: 3533.4533\n",
      "Epoch 432/1000\n",
      "7458/7458 [==============================] - 1s 123us/step - loss: 1516.6950 - val_loss: 3532.2492\n",
      "Epoch 433/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1518.3445 - val_loss: 3534.9085\n",
      "Epoch 434/1000\n",
      "7458/7458 [==============================] - 1s 119us/step - loss: 1520.8252 - val_loss: 3534.8094\n",
      "Epoch 435/1000\n",
      "7458/7458 [==============================] - 1s 128us/step - loss: 1518.9476 - val_loss: 3532.1160\n",
      "Epoch 436/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1520.9080 - val_loss: 3532.3263\n",
      "Epoch 437/1000\n",
      "7458/7458 [==============================] - 1s 127us/step - loss: 1515.5964 - val_loss: 3529.7221\n",
      "Epoch 438/1000\n",
      "7458/7458 [==============================] - 1s 126us/step - loss: 1516.2374 - val_loss: 3530.9095\n",
      "Epoch 439/1000\n",
      "7458/7458 [==============================] - 1s 124us/step - loss: 1515.6542 - val_loss: 3528.0286\n",
      "Epoch 440/1000\n",
      "7458/7458 [==============================] - 1s 129us/step - loss: 1513.6507 - val_loss: 3527.6780\n",
      "Epoch 441/1000\n",
      "7458/7458 [==============================] - 1s 128us/step - loss: 1516.8543 - val_loss: 3525.9112\n",
      "Epoch 442/1000\n",
      "7458/7458 [==============================] - 1s 123us/step - loss: 1518.8898 - val_loss: 3530.4727\n",
      "Epoch 443/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1520.0745 - val_loss: 3525.4111\n",
      "Epoch 444/1000\n",
      "7458/7458 [==============================] - 1s 122us/step - loss: 1514.0282 - val_loss: 3526.4143\n",
      "Epoch 445/1000\n",
      "7458/7458 [==============================] - 1s 130us/step - loss: 1513.2435 - val_loss: 3523.9115\n",
      "Epoch 446/1000\n",
      "7458/7458 [==============================] - 1s 126us/step - loss: 1514.3216 - val_loss: 3523.0581\n",
      "Epoch 447/1000\n",
      "7458/7458 [==============================] - 1s 127us/step - loss: 1515.9203 - val_loss: 3524.6262\n",
      "Epoch 448/1000\n",
      "7458/7458 [==============================] - 1s 126us/step - loss: 1512.8293 - val_loss: 3520.9079\n",
      "Epoch 449/1000\n",
      "7458/7458 [==============================] - 1s 123us/step - loss: 1512.3658 - val_loss: 3523.4607\n",
      "Epoch 450/1000\n",
      "7458/7458 [==============================] - 1s 129us/step - loss: 1512.0695 - val_loss: 3521.3422\n",
      "Epoch 451/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1511.9327 - val_loss: 3519.3141\n",
      "Epoch 452/1000\n",
      "7458/7458 [==============================] - 1s 129us/step - loss: 1514.6847 - val_loss: 3523.7024\n",
      "Epoch 453/1000\n",
      "7458/7458 [==============================] - 1s 127us/step - loss: 1516.3556 - val_loss: 3518.3305\n",
      "Epoch 454/1000\n",
      "7458/7458 [==============================] - 1s 124us/step - loss: 1511.3740 - val_loss: 3518.8681\n",
      "Epoch 455/1000\n",
      "7458/7458 [==============================] - 1s 128us/step - loss: 1511.2724 - val_loss: 3515.6493\n",
      "Epoch 456/1000\n",
      "7458/7458 [==============================] - 1s 124us/step - loss: 1512.8990 - val_loss: 3520.0663\n",
      "Epoch 457/1000\n",
      "7458/7458 [==============================] - 1s 124us/step - loss: 1516.2800 - val_loss: 3518.2394\n",
      "Epoch 458/1000\n",
      "7458/7458 [==============================] - 1s 124us/step - loss: 1517.8570 - val_loss: 3516.3755\n",
      "Epoch 459/1000\n",
      "7458/7458 [==============================] - 1s 128us/step - loss: 1510.6841 - val_loss: 3514.1344\n",
      "Epoch 460/1000\n",
      "7458/7458 [==============================] - 1s 121us/step - loss: 1511.7538 - val_loss: 3513.3723\n",
      "Epoch 461/1000\n",
      "7458/7458 [==============================] - 1s 129us/step - loss: 1510.1319 - val_loss: 3515.6341\n",
      "Epoch 462/1000\n",
      "7458/7458 [==============================] - 1s 126us/step - loss: 1509.8191 - val_loss: 3511.9991\n",
      "Epoch 463/1000\n",
      "7458/7458 [==============================] - 1s 126us/step - loss: 1512.0170 - val_loss: 3514.6462\n",
      "Epoch 464/1000\n",
      "7458/7458 [==============================] - 1s 126us/step - loss: 1509.9679 - val_loss: 3512.5807\n",
      "Epoch 465/1000\n",
      "7458/7458 [==============================] - 1s 128us/step - loss: 1508.5174 - val_loss: 3510.7244\n",
      "Epoch 466/1000\n",
      "7458/7458 [==============================] - 1s 123us/step - loss: 1508.3404 - val_loss: 3508.5843\n",
      "Epoch 467/1000\n",
      "7458/7458 [==============================] - 1s 128us/step - loss: 1507.0951 - val_loss: 3509.7047\n",
      "Epoch 468/1000\n",
      "7458/7458 [==============================] - 1s 124us/step - loss: 1508.0746 - val_loss: 3506.2495\n",
      "Epoch 469/1000\n",
      "7458/7458 [==============================] - 1s 127us/step - loss: 1511.4757 - val_loss: 3511.6547\n",
      "Epoch 470/1000\n",
      "7458/7458 [==============================] - 1s 126us/step - loss: 1512.6052 - val_loss: 3506.6502\n",
      "Epoch 471/1000\n",
      "7458/7458 [==============================] - 1s 129us/step - loss: 1506.3546 - val_loss: 3504.3536\n",
      "Epoch 472/1000\n",
      "7458/7458 [==============================] - 1s 120us/step - loss: 1506.5165 - val_loss: 3506.4477\n",
      "Epoch 473/1000\n",
      "7458/7458 [==============================] - 1s 126us/step - loss: 1508.1398 - val_loss: 3504.0487\n",
      "Epoch 474/1000\n",
      "7458/7458 [==============================] - 1s 130us/step - loss: 1505.9518 - val_loss: 3504.1174\n",
      "Epoch 475/1000\n",
      "7458/7458 [==============================] - 1s 124us/step - loss: 1505.5330 - val_loss: 3502.7598\n",
      "Epoch 476/1000\n",
      "7458/7458 [==============================] - 1s 124us/step - loss: 1506.4800 - val_loss: 3504.0313\n",
      "Epoch 477/1000\n",
      "7458/7458 [==============================] - 1s 123us/step - loss: 1503.9346 - val_loss: 3499.8863\n",
      "Epoch 478/1000\n",
      "7458/7458 [==============================] - 1s 123us/step - loss: 1507.4631 - val_loss: 3501.2834\n",
      "Epoch 479/1000\n",
      "7458/7458 [==============================] - 1s 118us/step - loss: 1505.2576 - val_loss: 3499.3041\n",
      "Epoch 480/1000\n",
      "7458/7458 [==============================] - 1s 123us/step - loss: 1507.1312 - val_loss: 3497.8836\n",
      "Epoch 481/1000\n",
      "7458/7458 [==============================] - 1s 122us/step - loss: 1505.8093 - val_loss: 3502.9018\n",
      "Epoch 482/1000\n",
      "7458/7458 [==============================] - 1s 126us/step - loss: 1509.2639 - val_loss: 3501.3004\n",
      "Epoch 483/1000\n",
      "7458/7458 [==============================] - 1s 127us/step - loss: 1504.7108 - val_loss: 3495.8108\n",
      "Epoch 484/1000\n",
      "7458/7458 [==============================] - 1s 129us/step - loss: 1510.2650 - val_loss: 3511.5982\n",
      "Epoch 485/1000\n",
      "7458/7458 [==============================] - 1s 125us/step - loss: 1506.8184 - val_loss: 3496.0098\n",
      "Epoch 486/1000\n",
      "7458/7458 [==============================] - 1s 122us/step - loss: 1505.2589 - val_loss: 3494.6142\n",
      "Epoch 00486: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f124cacfbe0>"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = buildManOneModel(X_train.shape)\n",
    "callback = EarlyStopping(monitor=\"loss\", patience=10, verbose=1, mode=\"auto\")\n",
    "model.fit(X_train, Y_train, epochs=1000, batch_size=128, validation_data=(X_val, Y_val), callbacks=[callback])yTo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7438 samples, validate on 1312 samples\n",
      "Epoch 1/1000\n",
      "7438/7438 [==============================] - 1s 133us/step - loss: 7143.4597 - val_loss: 5077.6538\n",
      "Epoch 2/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 7116.7282 - val_loss: 5071.9470\n",
      "Epoch 3/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 7102.4389 - val_loss: 5056.3140\n",
      "Epoch 4/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 7087.6622 - val_loss: 5055.2679\n",
      "Epoch 5/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 7075.3636 - val_loss: 5047.3354\n",
      "Epoch 6/1000\n",
      "7438/7438 [==============================] - 1s 122us/step - loss: 7068.2148 - val_loss: 5033.4740\n",
      "Epoch 7/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 7057.7696 - val_loss: 5034.3617\n",
      "Epoch 8/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 7048.7403 - val_loss: 5029.9201\n",
      "Epoch 9/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 7039.9550 - val_loss: 5026.1653\n",
      "Epoch 10/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 7033.1504 - val_loss: 5017.1572\n",
      "Epoch 11/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 7028.0502 - val_loss: 5018.0320\n",
      "Epoch 12/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 7017.9874 - val_loss: 5016.9745\n",
      "Epoch 13/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 7013.4290 - val_loss: 5010.8536\n",
      "Epoch 14/1000\n",
      "7438/7438 [==============================] - 1s 121us/step - loss: 7007.2435 - val_loss: 4997.9234\n",
      "Epoch 15/1000\n",
      "7438/7438 [==============================] - 1s 122us/step - loss: 7000.5296 - val_loss: 4996.7347\n",
      "Epoch 16/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 6991.9634 - val_loss: 4993.3598\n",
      "Epoch 17/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 6984.7024 - val_loss: 4991.2877\n",
      "Epoch 18/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 6977.3134 - val_loss: 4987.0841\n",
      "Epoch 19/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 6972.5523 - val_loss: 4989.6472\n",
      "Epoch 20/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 6969.1141 - val_loss: 4984.5116\n",
      "Epoch 21/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 6960.8509 - val_loss: 4971.9752\n",
      "Epoch 22/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 6951.9643 - val_loss: 4968.1572\n",
      "Epoch 23/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 6944.2705 - val_loss: 4964.4147\n",
      "Epoch 24/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 6938.3943 - val_loss: 4962.3573\n",
      "Epoch 25/1000\n",
      "7438/7438 [==============================] - 1s 132us/step - loss: 6932.9130 - val_loss: 4958.3756\n",
      "Epoch 26/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 6924.9896 - val_loss: 4955.9223\n",
      "Epoch 27/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 6919.6662 - val_loss: 4951.4689\n",
      "Epoch 28/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 6913.4748 - val_loss: 4951.5884\n",
      "Epoch 29/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 6907.0472 - val_loss: 4946.8435\n",
      "Epoch 30/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 6901.3071 - val_loss: 4942.5769\n",
      "Epoch 31/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 6895.3730 - val_loss: 4939.8763\n",
      "Epoch 32/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 6889.4202 - val_loss: 4937.4890\n",
      "Epoch 33/1000\n",
      "7438/7438 [==============================] - 1s 131us/step - loss: 6882.9192 - val_loss: 4934.3820\n",
      "Epoch 34/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 6878.9385 - val_loss: 4931.6752\n",
      "Epoch 35/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 6872.0367 - val_loss: 4932.8265\n",
      "Epoch 36/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 6866.8957 - val_loss: 4925.4510\n",
      "Epoch 37/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 6864.0475 - val_loss: 4923.9729\n",
      "Epoch 38/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 6856.3895 - val_loss: 4915.5332\n",
      "Epoch 39/1000\n",
      "7438/7438 [==============================] - 1s 122us/step - loss: 6849.1757 - val_loss: 4915.0057\n",
      "Epoch 40/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 6844.1824 - val_loss: 4907.8668\n",
      "Epoch 41/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 6839.2811 - val_loss: 4907.4037\n",
      "Epoch 42/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 6833.8147 - val_loss: 4905.2455\n",
      "Epoch 43/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 6828.1974 - val_loss: 4897.1600\n",
      "Epoch 44/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 6824.8242 - val_loss: 4898.7401\n",
      "Epoch 45/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 6817.7592 - val_loss: 4892.1262\n",
      "Epoch 46/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 6812.2257 - val_loss: 4891.8142\n",
      "Epoch 47/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 6805.5374 - val_loss: 4883.1061\n",
      "Epoch 48/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 6799.8443 - val_loss: 4881.9547\n",
      "Epoch 49/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 6794.8896 - val_loss: 4878.4687\n",
      "Epoch 50/1000\n",
      "7438/7438 [==============================] - 1s 121us/step - loss: 6790.9929 - val_loss: 4878.2054\n",
      "Epoch 51/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 6786.1020 - val_loss: 4873.8372\n",
      "Epoch 52/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 6780.4577 - val_loss: 4868.4720\n",
      "Epoch 53/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 6773.4662 - val_loss: 4869.3794\n",
      "Epoch 54/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 6768.1536 - val_loss: 4862.8463\n",
      "Epoch 55/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 6764.3846 - val_loss: 4858.9988\n",
      "Epoch 56/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 6758.6967 - val_loss: 4855.7716\n",
      "Epoch 57/1000\n",
      "7438/7438 [==============================] - 1s 131us/step - loss: 6752.9324 - val_loss: 4851.5856\n",
      "Epoch 58/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 6747.6775 - val_loss: 4852.4605\n",
      "Epoch 59/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 6742.8440 - val_loss: 4845.2796\n",
      "Epoch 60/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 6738.3523 - val_loss: 4842.5636\n",
      "Epoch 61/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 6733.3894 - val_loss: 4841.4589\n",
      "Epoch 62/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 6757.0991 - val_loss: 4859.7661\n",
      "Epoch 63/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 6730.2332 - val_loss: 4843.1289\n",
      "Epoch 64/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 6723.3226 - val_loss: 4837.0965\n",
      "Epoch 65/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 6717.4038 - val_loss: 4832.4407\n",
      "Epoch 66/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 6711.3097 - val_loss: 4828.5213\n",
      "Epoch 67/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 6704.7378 - val_loss: 4826.6943\n",
      "Epoch 68/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 6702.4464 - val_loss: 4823.0399\n",
      "Epoch 69/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 6695.0440 - val_loss: 4818.2829\n",
      "Epoch 70/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 6689.1917 - val_loss: 4816.4674\n",
      "Epoch 71/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 6685.8454 - val_loss: 4812.5336\n",
      "Epoch 72/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 6678.8866 - val_loss: 4811.9209\n",
      "Epoch 73/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 6674.7762 - val_loss: 4807.1130\n",
      "Epoch 74/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 6670.6944 - val_loss: 4803.5401\n",
      "Epoch 75/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 6665.6443 - val_loss: 4799.9433\n",
      "Epoch 76/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 6661.1883 - val_loss: 4799.1665\n",
      "Epoch 77/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 6656.4800 - val_loss: 4796.3276\n",
      "Epoch 78/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 6652.6625 - val_loss: 4792.4911\n",
      "Epoch 79/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 6647.6152 - val_loss: 4788.3428\n",
      "Epoch 80/1000\n",
      "7438/7438 [==============================] - 1s 131us/step - loss: 6644.1286 - val_loss: 4786.3402\n",
      "Epoch 81/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 6639.0762 - val_loss: 4784.4638\n",
      "Epoch 82/1000\n",
      "7438/7438 [==============================] - 1s 132us/step - loss: 6634.7048 - val_loss: 4780.2238\n",
      "Epoch 83/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 6629.8744 - val_loss: 4778.4798\n",
      "Epoch 84/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 6624.7431 - val_loss: 4775.1408\n",
      "Epoch 85/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 6620.1461 - val_loss: 4775.6807\n",
      "Epoch 86/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 6615.5745 - val_loss: 4769.5961\n",
      "Epoch 87/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 6612.5877 - val_loss: 4768.8724\n",
      "Epoch 88/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 6607.4154 - val_loss: 4764.5420\n",
      "Epoch 89/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 6603.3134 - val_loss: 4761.6917\n",
      "Epoch 90/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 6598.7506 - val_loss: 4758.7408\n",
      "Epoch 91/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 6593.9184 - val_loss: 4755.7367\n",
      "Epoch 92/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 6590.5548 - val_loss: 4757.1908\n",
      "Epoch 93/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 6584.9729 - val_loss: 4749.7864\n",
      "Epoch 94/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 6578.6044 - val_loss: 4746.2888\n",
      "Epoch 95/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 6574.5665 - val_loss: 4745.0634\n",
      "Epoch 96/1000\n",
      "7438/7438 [==============================] - 1s 121us/step - loss: 6571.9590 - val_loss: 4742.3970\n",
      "Epoch 97/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 6567.2482 - val_loss: 4738.0559\n",
      "Epoch 98/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 6561.2371 - val_loss: 4734.2021\n",
      "Epoch 99/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 6556.2442 - val_loss: 4732.3200\n",
      "Epoch 100/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 6552.9902 - val_loss: 4733.2902\n",
      "Epoch 101/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 6549.2127 - val_loss: 4728.9451\n",
      "Epoch 102/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 6543.0072 - val_loss: 4730.3113\n",
      "Epoch 103/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 6539.4446 - val_loss: 4720.7438\n",
      "Epoch 104/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 6533.0453 - val_loss: 4720.5311\n",
      "Epoch 105/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 6530.1003 - val_loss: 4715.5963\n",
      "Epoch 106/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 6525.3524 - val_loss: 4716.1643\n",
      "Epoch 107/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 6521.9004 - val_loss: 4711.4971\n",
      "Epoch 108/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 6516.4164 - val_loss: 4709.7888\n",
      "Epoch 109/1000\n",
      "7438/7438 [==============================] - 1s 122us/step - loss: 6514.7071 - val_loss: 4704.3909\n",
      "Epoch 110/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 6510.6119 - val_loss: 4702.9359\n",
      "Epoch 111/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 6505.8109 - val_loss: 4700.5838\n",
      "Epoch 112/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 6503.9049 - val_loss: 4706.2382\n",
      "Epoch 113/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 6493.8426 - val_loss: 4694.5288\n",
      "Epoch 114/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 6491.8200 - val_loss: 4695.3408\n",
      "Epoch 115/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 6486.2027 - val_loss: 4689.0285\n",
      "Epoch 116/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 6482.2497 - val_loss: 4686.1086\n",
      "Epoch 117/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 6477.9597 - val_loss: 4686.2126\n",
      "Epoch 118/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 6472.8373 - val_loss: 4681.8423\n",
      "Epoch 119/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 6467.8208 - val_loss: 4678.5145\n",
      "Epoch 120/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 6463.7743 - val_loss: 4677.8881\n",
      "Epoch 121/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 6460.1353 - val_loss: 4674.8688\n",
      "Epoch 122/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 6464.1609 - val_loss: 4672.0335\n",
      "Epoch 123/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 6452.1663 - val_loss: 4667.5354\n",
      "Epoch 124/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 6446.9493 - val_loss: 4664.8115\n",
      "Epoch 125/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 6443.3156 - val_loss: 4658.5122\n",
      "Epoch 126/1000\n",
      "7438/7438 [==============================] - 1s 121us/step - loss: 6437.5101 - val_loss: 4662.8428\n",
      "Epoch 127/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 6435.8209 - val_loss: 4653.5378\n",
      "Epoch 128/1000\n",
      "7438/7438 [==============================] - 1s 122us/step - loss: 6426.9037 - val_loss: 4653.8665\n",
      "Epoch 129/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 6422.9049 - val_loss: 4649.9226\n",
      "Epoch 130/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 6422.8777 - val_loss: 4650.8284\n",
      "Epoch 131/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 6415.8438 - val_loss: 4645.1317\n",
      "Epoch 132/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 6412.6795 - val_loss: 4640.2333\n",
      "Epoch 133/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 6442.6993 - val_loss: 4657.9154\n",
      "Epoch 134/1000\n",
      "7438/7438 [==============================] - 1s 121us/step - loss: 6414.2112 - val_loss: 4640.7827\n",
      "Epoch 135/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 6401.7877 - val_loss: 4634.2573\n",
      "Epoch 136/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 6393.3850 - val_loss: 4633.3721\n",
      "Epoch 137/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 6389.2066 - val_loss: 4626.8099\n",
      "Epoch 138/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 6383.7691 - val_loss: 4624.2021\n",
      "Epoch 139/1000\n",
      "7438/7438 [==============================] - 1s 121us/step - loss: 6381.3112 - val_loss: 4626.7744\n",
      "Epoch 140/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 6376.7470 - val_loss: 4619.4840\n",
      "Epoch 141/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 6370.4829 - val_loss: 4617.5093\n",
      "Epoch 142/1000\n",
      "7438/7438 [==============================] - 1s 131us/step - loss: 6365.7479 - val_loss: 4614.2326\n",
      "Epoch 143/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 6359.2583 - val_loss: 4609.6842\n",
      "Epoch 144/1000\n",
      "7438/7438 [==============================] - 1s 119us/step - loss: 6358.7299 - val_loss: 4607.2627\n",
      "Epoch 145/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 6352.8385 - val_loss: 4605.1267\n",
      "Epoch 146/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 6349.5253 - val_loss: 4603.4753\n",
      "Epoch 147/1000\n",
      "7438/7438 [==============================] - 1s 121us/step - loss: 6343.8453 - val_loss: 4602.0416\n",
      "Epoch 148/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 6339.2616 - val_loss: 4595.6060\n",
      "Epoch 149/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 6333.3998 - val_loss: 4597.8250\n",
      "Epoch 150/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 6333.0147 - val_loss: 4593.0324\n",
      "Epoch 151/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 6325.4230 - val_loss: 4598.1607\n",
      "Epoch 152/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 6319.7056 - val_loss: 4587.1604\n",
      "Epoch 153/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 6312.9358 - val_loss: 4584.6736\n",
      "Epoch 154/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 6309.0769 - val_loss: 4581.1269\n",
      "Epoch 155/1000\n",
      "7438/7438 [==============================] - 1s 120us/step - loss: 6302.7631 - val_loss: 4577.3136\n",
      "Epoch 156/1000\n",
      "7438/7438 [==============================] - 1s 118us/step - loss: 6297.5197 - val_loss: 4572.6014\n",
      "Epoch 157/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 6290.9822 - val_loss: 4570.5966\n",
      "Epoch 158/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 6286.5822 - val_loss: 4567.3996\n",
      "Epoch 159/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 6280.9098 - val_loss: 4562.0059\n",
      "Epoch 160/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 6277.8027 - val_loss: 4559.3511\n",
      "Epoch 161/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 6274.7819 - val_loss: 4559.4502\n",
      "Epoch 162/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 6268.7621 - val_loss: 4555.7011\n",
      "Epoch 163/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 6266.1317 - val_loss: 4553.0592\n",
      "Epoch 164/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 6263.2499 - val_loss: 4550.8366\n",
      "Epoch 165/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 6255.7209 - val_loss: 4547.9386\n",
      "Epoch 166/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 6253.7360 - val_loss: 4544.8117\n",
      "Epoch 167/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 6247.6237 - val_loss: 4539.6054\n",
      "Epoch 168/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 6248.5503 - val_loss: 4540.0992\n",
      "Epoch 169/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 6237.6611 - val_loss: 4532.4490\n",
      "Epoch 170/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 6235.0051 - val_loss: 4537.3848\n",
      "Epoch 171/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 6231.1395 - val_loss: 4530.6216\n",
      "Epoch 172/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 6237.0998 - val_loss: 4530.2710\n",
      "Epoch 173/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 6223.4664 - val_loss: 4523.5958\n",
      "Epoch 174/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 6217.1490 - val_loss: 4525.6543\n",
      "Epoch 175/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 6213.0599 - val_loss: 4520.9185\n",
      "Epoch 176/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 6207.9732 - val_loss: 4517.5992\n",
      "Epoch 177/1000\n",
      "7438/7438 [==============================] - 1s 121us/step - loss: 6204.2325 - val_loss: 4514.1853\n",
      "Epoch 178/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 6199.3871 - val_loss: 4511.4736\n",
      "Epoch 179/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 6195.3248 - val_loss: 4509.9193\n",
      "Epoch 180/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 6190.9908 - val_loss: 4508.1270\n",
      "Epoch 181/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 6187.7355 - val_loss: 4502.4547\n",
      "Epoch 182/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 6183.8653 - val_loss: 4507.4415\n",
      "Epoch 183/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 6180.5677 - val_loss: 4496.3049\n",
      "Epoch 184/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 6176.5494 - val_loss: 4496.8019\n",
      "Epoch 185/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 6171.5424 - val_loss: 4491.7693\n",
      "Epoch 186/1000\n",
      "7438/7438 [==============================] - 1s 131us/step - loss: 6165.5242 - val_loss: 4495.0280\n",
      "Epoch 187/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 6162.2606 - val_loss: 4483.8871\n",
      "Epoch 188/1000\n",
      "7438/7438 [==============================] - 1s 121us/step - loss: 6158.6798 - val_loss: 4490.4078\n",
      "Epoch 189/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 6154.2189 - val_loss: 4481.3728\n",
      "Epoch 190/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 6147.2834 - val_loss: 4476.3369\n",
      "Epoch 191/1000\n",
      "7438/7438 [==============================] - 1s 120us/step - loss: 6146.0289 - val_loss: 4485.1531\n",
      "Epoch 192/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 6142.6693 - val_loss: 4471.3617\n",
      "Epoch 193/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 6137.1836 - val_loss: 4472.4177\n",
      "Epoch 194/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 6134.6290 - val_loss: 4469.2216\n",
      "Epoch 195/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 6131.2052 - val_loss: 4464.6068\n",
      "Epoch 196/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 6127.8551 - val_loss: 4465.2959\n",
      "Epoch 197/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 6122.1531 - val_loss: 4457.6027\n",
      "Epoch 198/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 6115.2467 - val_loss: 4460.7761\n",
      "Epoch 199/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 6109.8385 - val_loss: 4455.0193\n",
      "Epoch 200/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 6106.0115 - val_loss: 4458.4054\n",
      "Epoch 201/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 6103.5056 - val_loss: 4457.5394\n",
      "Epoch 202/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 6098.1122 - val_loss: 4443.9610\n",
      "Epoch 203/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 6093.1002 - val_loss: 4449.6073\n",
      "Epoch 204/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 6092.2522 - val_loss: 4441.6744\n",
      "Epoch 205/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 6084.5814 - val_loss: 4436.5718\n",
      "Epoch 206/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 6078.7690 - val_loss: 4437.2838\n",
      "Epoch 207/1000\n",
      "7438/7438 [==============================] - 1s 131us/step - loss: 6077.0953 - val_loss: 4431.9845\n",
      "Epoch 208/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 6083.6760 - val_loss: 4447.2507\n",
      "Epoch 209/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 6073.4285 - val_loss: 4428.7850\n",
      "Epoch 210/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 6063.4830 - val_loss: 4428.1255\n",
      "Epoch 211/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 6062.0676 - val_loss: 4420.0562\n",
      "Epoch 212/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 6061.0057 - val_loss: 4420.5579\n",
      "Epoch 213/1000\n",
      "7438/7438 [==============================] - 1s 122us/step - loss: 6054.9884 - val_loss: 4424.4179\n",
      "Epoch 214/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 6049.5031 - val_loss: 4412.8953\n",
      "Epoch 215/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 6042.8955 - val_loss: 4413.7040\n",
      "Epoch 216/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 6039.8048 - val_loss: 4407.8848\n",
      "Epoch 217/1000\n",
      "7438/7438 [==============================] - 1s 132us/step - loss: 6036.6351 - val_loss: 4403.2323\n",
      "Epoch 218/1000\n",
      "7438/7438 [==============================] - 1s 122us/step - loss: 6031.3229 - val_loss: 4406.2212\n",
      "Epoch 219/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 6027.4470 - val_loss: 4408.2901\n",
      "Epoch 220/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 6019.5542 - val_loss: 4401.9610\n",
      "Epoch 221/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 6013.1002 - val_loss: 4394.6574\n",
      "Epoch 222/1000\n",
      "7438/7438 [==============================] - 1s 132us/step - loss: 6011.4103 - val_loss: 4394.8889\n",
      "Epoch 223/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 6007.0664 - val_loss: 4391.6698\n",
      "Epoch 224/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 6000.1725 - val_loss: 4386.5675\n",
      "Epoch 225/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 5994.2942 - val_loss: 4387.4243\n",
      "Epoch 226/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 5989.6594 - val_loss: 4383.9340\n",
      "Epoch 227/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 5988.5573 - val_loss: 4381.0181\n",
      "Epoch 228/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 5981.4914 - val_loss: 4381.3483\n",
      "Epoch 229/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 5976.1058 - val_loss: 4370.3718\n",
      "Epoch 230/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 5981.0154 - val_loss: 4385.4935\n",
      "Epoch 231/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 5970.5445 - val_loss: 4371.9935\n",
      "Epoch 232/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 5965.5805 - val_loss: 4370.7509\n",
      "Epoch 233/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 5964.1069 - val_loss: 4369.3319\n",
      "Epoch 234/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 5960.6259 - val_loss: 4369.8691\n",
      "Epoch 235/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 5956.5076 - val_loss: 4369.4436\n",
      "Epoch 236/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 5949.1325 - val_loss: 4358.3114\n",
      "Epoch 237/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 5941.9886 - val_loss: 4354.5165\n",
      "Epoch 238/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 5940.1794 - val_loss: 4354.0295\n",
      "Epoch 239/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 5937.4394 - val_loss: 4354.3385\n",
      "Epoch 240/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 5932.0952 - val_loss: 4345.1070\n",
      "Epoch 241/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 5928.6419 - val_loss: 4353.3498\n",
      "Epoch 242/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 5925.3659 - val_loss: 4345.0776\n",
      "Epoch 243/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 5920.3316 - val_loss: 4343.5600\n",
      "Epoch 244/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 5917.2574 - val_loss: 4340.0331\n",
      "Epoch 245/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 5911.0171 - val_loss: 4333.7793\n",
      "Epoch 246/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 5904.9373 - val_loss: 4334.7087\n",
      "Epoch 247/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 5904.1998 - val_loss: 4336.1809\n",
      "Epoch 248/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 5897.9789 - val_loss: 4330.1694\n",
      "Epoch 249/1000\n",
      "7438/7438 [==============================] - 1s 122us/step - loss: 5898.9671 - val_loss: 4321.7248\n",
      "Epoch 250/1000\n",
      "7438/7438 [==============================] - 1s 132us/step - loss: 5890.3208 - val_loss: 4324.3286\n",
      "Epoch 251/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 5887.5278 - val_loss: 4315.8558\n",
      "Epoch 252/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 5885.2957 - val_loss: 4320.2134\n",
      "Epoch 253/1000\n",
      "7438/7438 [==============================] - 1s 122us/step - loss: 5880.4385 - val_loss: 4316.3442\n",
      "Epoch 254/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 5874.3351 - val_loss: 4310.2667\n",
      "Epoch 255/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 5870.1939 - val_loss: 4306.6631\n",
      "Epoch 256/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 5868.6904 - val_loss: 4303.0743\n",
      "Epoch 257/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 5862.8603 - val_loss: 4301.4487\n",
      "Epoch 258/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 5859.1117 - val_loss: 4297.4545\n",
      "Epoch 259/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 5859.4995 - val_loss: 4306.5055\n",
      "Epoch 260/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 5851.6202 - val_loss: 4299.7162\n",
      "Epoch 261/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 5844.7988 - val_loss: 4289.2788\n",
      "Epoch 262/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 5840.4137 - val_loss: 4290.2623\n",
      "Epoch 263/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 5840.3590 - val_loss: 4297.1324\n",
      "Epoch 264/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 5833.9451 - val_loss: 4285.1491\n",
      "Epoch 265/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 5829.7653 - val_loss: 4283.6058\n",
      "Epoch 266/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 5826.1052 - val_loss: 4285.1257\n",
      "Epoch 267/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 5824.0067 - val_loss: 4279.4938\n",
      "Epoch 268/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 5817.3071 - val_loss: 4275.5956\n",
      "Epoch 269/1000\n",
      "7438/7438 [==============================] - 1s 133us/step - loss: 5813.0638 - val_loss: 4271.1475\n",
      "Epoch 270/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 5807.0000 - val_loss: 4264.7688\n",
      "Epoch 271/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 5813.4319 - val_loss: 4266.8206\n",
      "Epoch 272/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 5800.6786 - val_loss: 4264.1592\n",
      "Epoch 273/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 5797.8943 - val_loss: 4259.5648\n",
      "Epoch 274/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 5791.6383 - val_loss: 4256.8970\n",
      "Epoch 275/1000\n",
      "7438/7438 [==============================] - 1s 131us/step - loss: 5790.9362 - val_loss: 4255.7550\n",
      "Epoch 276/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 5788.6855 - val_loss: 4256.9100\n",
      "Epoch 277/1000\n",
      "7438/7438 [==============================] - 1s 121us/step - loss: 5794.0012 - val_loss: 4273.2621\n",
      "Epoch 278/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 5780.4697 - val_loss: 4249.5119\n",
      "Epoch 279/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 5776.5817 - val_loss: 4249.0453\n",
      "Epoch 280/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 5775.7781 - val_loss: 4237.8928\n",
      "Epoch 281/1000\n",
      "7438/7438 [==============================] - 1s 131us/step - loss: 5771.0203 - val_loss: 4246.1651\n",
      "Epoch 282/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 5765.0484 - val_loss: 4240.6066\n",
      "Epoch 283/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 5757.6950 - val_loss: 4233.3315\n",
      "Epoch 284/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 5754.6944 - val_loss: 4230.6718\n",
      "Epoch 285/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 5746.2071 - val_loss: 4227.3590\n",
      "Epoch 286/1000\n",
      "7438/7438 [==============================] - 1s 119us/step - loss: 5743.2615 - val_loss: 4222.5631\n",
      "Epoch 287/1000\n",
      "7438/7438 [==============================] - 1s 118us/step - loss: 5740.4428 - val_loss: 4227.7086\n",
      "Epoch 288/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 5734.8339 - val_loss: 4218.6242\n",
      "Epoch 289/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 5731.2518 - val_loss: 4217.5326\n",
      "Epoch 290/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 5726.3853 - val_loss: 4227.6393\n",
      "Epoch 291/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 5722.6653 - val_loss: 4212.0269\n",
      "Epoch 292/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 5719.4960 - val_loss: 4211.6317\n",
      "Epoch 293/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 5714.6134 - val_loss: 4210.6981\n",
      "Epoch 294/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 5713.2411 - val_loss: 4215.0886\n",
      "Epoch 295/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 5709.4965 - val_loss: 4207.0563\n",
      "Epoch 296/1000\n",
      "7438/7438 [==============================] - 1s 122us/step - loss: 5701.8560 - val_loss: 4196.6716\n",
      "Epoch 297/1000\n",
      "7438/7438 [==============================] - 1s 120us/step - loss: 5697.7288 - val_loss: 4201.8467\n",
      "Epoch 298/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 5695.3986 - val_loss: 4193.2819\n",
      "Epoch 299/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 5696.4314 - val_loss: 4191.0908\n",
      "Epoch 300/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 5685.8136 - val_loss: 4186.7399\n",
      "Epoch 301/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 5684.2865 - val_loss: 4181.9383\n",
      "Epoch 302/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 5677.9828 - val_loss: 4187.2324\n",
      "Epoch 303/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 5678.7250 - val_loss: 4183.7814\n",
      "Epoch 304/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 5671.2409 - val_loss: 4176.1120\n",
      "Epoch 305/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 5667.6323 - val_loss: 4173.4995\n",
      "Epoch 306/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 5663.8293 - val_loss: 4191.0591\n",
      "Epoch 307/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 5689.9427 - val_loss: 4196.5411\n",
      "Epoch 308/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 5672.6570 - val_loss: 4187.5601\n",
      "Epoch 309/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 5662.4908 - val_loss: 4173.7099\n",
      "Epoch 310/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 5651.2318 - val_loss: 4165.1505\n",
      "Epoch 311/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 5643.0172 - val_loss: 4160.0350\n",
      "Epoch 312/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 5648.2753 - val_loss: 4171.7913\n",
      "Epoch 313/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 5636.4578 - val_loss: 4156.1051\n",
      "Epoch 314/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 5636.9506 - val_loss: 4156.4351\n",
      "Epoch 315/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 5627.4211 - val_loss: 4153.2543\n",
      "Epoch 316/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 5623.5326 - val_loss: 4149.4471\n",
      "Epoch 317/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 5616.0912 - val_loss: 4148.1382\n",
      "Epoch 318/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 5614.5905 - val_loss: 4141.1006\n",
      "Epoch 319/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 5614.0541 - val_loss: 4141.0180\n",
      "Epoch 320/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 5607.2004 - val_loss: 4134.8586\n",
      "Epoch 321/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 5604.9646 - val_loss: 4137.4879\n",
      "Epoch 322/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 5600.8730 - val_loss: 4133.2787\n",
      "Epoch 323/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 5595.5460 - val_loss: 4127.9759\n",
      "Epoch 324/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 5594.9925 - val_loss: 4131.6586\n",
      "Epoch 325/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 5585.8923 - val_loss: 4129.9076\n",
      "Epoch 326/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 5584.0983 - val_loss: 4124.6594\n",
      "Epoch 327/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 5581.0661 - val_loss: 4128.7969\n",
      "Epoch 328/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 5576.0373 - val_loss: 4115.2065\n",
      "Epoch 329/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 5594.9888 - val_loss: 4579.7279\n",
      "Epoch 330/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 5812.6382 - val_loss: 4116.4347\n",
      "Epoch 331/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 5565.2117 - val_loss: 4108.0270\n",
      "Epoch 332/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 5561.6211 - val_loss: 4108.1621\n",
      "Epoch 333/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 5571.4023 - val_loss: 4124.5517\n",
      "Epoch 334/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 5559.6157 - val_loss: 4105.6658\n",
      "Epoch 335/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 5558.2815 - val_loss: 4104.6698\n",
      "Epoch 336/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 5547.3233 - val_loss: 4107.5502\n",
      "Epoch 337/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 5543.2464 - val_loss: 4095.3027\n",
      "Epoch 338/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 5537.6235 - val_loss: 4102.0860\n",
      "Epoch 339/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 5536.1352 - val_loss: 4092.4536\n",
      "Epoch 340/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 5534.3834 - val_loss: 4094.7917\n",
      "Epoch 341/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 5532.3441 - val_loss: 4095.4980\n",
      "Epoch 342/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 5525.3353 - val_loss: 4083.6268\n",
      "Epoch 343/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 5517.5499 - val_loss: 4079.8461\n",
      "Epoch 344/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 5512.6073 - val_loss: 4079.3056\n",
      "Epoch 345/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 5506.7557 - val_loss: 4072.2085\n",
      "Epoch 346/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 5503.1382 - val_loss: 4073.4457\n",
      "Epoch 347/1000\n",
      "7438/7438 [==============================] - 1s 131us/step - loss: 5500.8379 - val_loss: 4067.3183\n",
      "Epoch 348/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 5498.8318 - val_loss: 4068.7058\n",
      "Epoch 349/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 5497.6871 - val_loss: 4077.5488\n",
      "Epoch 350/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 5495.7486 - val_loss: 4072.3255\n",
      "Epoch 351/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 5492.5434 - val_loss: 4063.4742\n",
      "Epoch 352/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 5482.7759 - val_loss: 4066.8113\n",
      "Epoch 353/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 5478.3363 - val_loss: 4052.7441\n",
      "Epoch 354/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 5471.3278 - val_loss: 4055.0693\n",
      "Epoch 355/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 5475.4573 - val_loss: 4068.6568\n",
      "Epoch 356/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 5473.6550 - val_loss: 4050.3998\n",
      "Epoch 357/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 5469.0523 - val_loss: 4052.8608\n",
      "Epoch 358/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 5462.6803 - val_loss: 4041.7938\n",
      "Epoch 359/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 5454.5261 - val_loss: 4041.8050\n",
      "Epoch 360/1000\n",
      "7438/7438 [==============================] - 1s 132us/step - loss: 5450.3110 - val_loss: 4035.4637\n",
      "Epoch 361/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 5448.4449 - val_loss: 4044.4807\n",
      "Epoch 362/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 5448.5259 - val_loss: 4033.9790\n",
      "Epoch 363/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 5446.2183 - val_loss: 4054.3464\n",
      "Epoch 364/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 5446.5335 - val_loss: 4033.8243\n",
      "Epoch 365/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 5428.9740 - val_loss: 4022.7399\n",
      "Epoch 366/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 5430.3617 - val_loss: 4026.4762\n",
      "Epoch 367/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 5429.6503 - val_loss: 4026.3377\n",
      "Epoch 368/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 5423.0445 - val_loss: 4024.6001\n",
      "Epoch 369/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 5416.3146 - val_loss: 4016.7294\n",
      "Epoch 370/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 5413.9805 - val_loss: 4036.1541\n",
      "Epoch 371/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 5419.5693 - val_loss: 4024.0557\n",
      "Epoch 372/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 5402.8283 - val_loss: 4007.0963\n",
      "Epoch 373/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 5401.0318 - val_loss: 4004.2541\n",
      "Epoch 374/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 5395.8869 - val_loss: 4005.8608\n",
      "Epoch 375/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 5394.3391 - val_loss: 4004.5367\n",
      "Epoch 376/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 5391.1267 - val_loss: 3999.7368\n",
      "Epoch 377/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 5384.0853 - val_loss: 3993.5298\n",
      "Epoch 378/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 5380.0859 - val_loss: 4000.2359\n",
      "Epoch 379/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 5375.2374 - val_loss: 3995.0833\n",
      "Epoch 380/1000\n",
      "7438/7438 [==============================] - 1s 121us/step - loss: 5372.1899 - val_loss: 3990.0183\n",
      "Epoch 381/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 5366.1217 - val_loss: 3986.0858\n",
      "Epoch 382/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 5363.9580 - val_loss: 3984.2017\n",
      "Epoch 383/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 5360.1033 - val_loss: 3990.4497\n",
      "Epoch 384/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 5356.0425 - val_loss: 3977.8245\n",
      "Epoch 385/1000\n",
      "7438/7438 [==============================] - 1s 131us/step - loss: 5354.9887 - val_loss: 3976.3527\n",
      "Epoch 386/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 5359.9220 - val_loss: 3980.0457\n",
      "Epoch 387/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 5362.3996 - val_loss: 3985.2377\n",
      "Epoch 388/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 5345.9812 - val_loss: 3971.6877\n",
      "Epoch 389/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 5339.2234 - val_loss: 3969.4151\n",
      "Epoch 390/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 5334.3568 - val_loss: 3973.0878\n",
      "Epoch 391/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 5337.9785 - val_loss: 3967.9010\n",
      "Epoch 392/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 5334.3843 - val_loss: 3969.9218\n",
      "Epoch 393/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 5341.4283 - val_loss: 3987.8746\n",
      "Epoch 394/1000\n",
      "7438/7438 [==============================] - 1s 133us/step - loss: 5325.0053 - val_loss: 3960.7182\n",
      "Epoch 395/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 5320.4966 - val_loss: 3961.1580\n",
      "Epoch 396/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 5314.3424 - val_loss: 3951.7023\n",
      "Epoch 397/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 5313.1119 - val_loss: 3958.7779\n",
      "Epoch 398/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 5306.2504 - val_loss: 3954.0415\n",
      "Epoch 399/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 5303.1582 - val_loss: 3944.3762\n",
      "Epoch 400/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 5299.4835 - val_loss: 3946.6325\n",
      "Epoch 401/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 5292.5635 - val_loss: 3941.3374\n",
      "Epoch 402/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 5290.7485 - val_loss: 3949.2371\n",
      "Epoch 403/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 5290.3259 - val_loss: 3938.5670\n",
      "Epoch 404/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 5284.5051 - val_loss: 3942.1322\n",
      "Epoch 405/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 5284.4344 - val_loss: 3933.8254\n",
      "Epoch 406/1000\n",
      "7438/7438 [==============================] - 1s 132us/step - loss: 5277.7426 - val_loss: 3928.1467\n",
      "Epoch 407/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 5291.4573 - val_loss: 3941.2219\n",
      "Epoch 408/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 5275.0824 - val_loss: 3929.5715\n",
      "Epoch 409/1000\n",
      "7438/7438 [==============================] - 1s 132us/step - loss: 5297.4426 - val_loss: 3940.8473\n",
      "Epoch 410/1000\n",
      "7438/7438 [==============================] - 1s 131us/step - loss: 5267.2577 - val_loss: 3923.3587\n",
      "Epoch 411/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 5259.5704 - val_loss: 3925.7089\n",
      "Epoch 412/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 5257.6308 - val_loss: 3939.7113\n",
      "Epoch 413/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 5251.9502 - val_loss: 3920.8848\n",
      "Epoch 414/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 5247.3503 - val_loss: 3910.6419\n",
      "Epoch 415/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 5241.5084 - val_loss: 3914.0961\n",
      "Epoch 416/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 5243.7260 - val_loss: 3920.4983\n",
      "Epoch 417/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 5241.2547 - val_loss: 3914.7665\n",
      "Epoch 418/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 5242.1271 - val_loss: 3914.8064\n",
      "Epoch 419/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 5238.0436 - val_loss: 3903.8154\n",
      "Epoch 420/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 5229.8049 - val_loss: 3904.6124\n",
      "Epoch 421/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 5231.1800 - val_loss: 3900.8546\n",
      "Epoch 422/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 5218.0668 - val_loss: 3893.9649\n",
      "Epoch 423/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 5218.7090 - val_loss: 3908.9688\n",
      "Epoch 424/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 5221.5828 - val_loss: 3902.1138\n",
      "Epoch 425/1000\n",
      "7438/7438 [==============================] - 1s 121us/step - loss: 5218.9151 - val_loss: 3892.6404\n",
      "Epoch 426/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 5208.1790 - val_loss: 3886.5967\n",
      "Epoch 427/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 5205.4486 - val_loss: 3887.5928\n",
      "Epoch 428/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 5200.5843 - val_loss: 3883.1966\n",
      "Epoch 429/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 5216.1661 - val_loss: 3907.9430\n",
      "Epoch 430/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 5200.9169 - val_loss: 3885.2337\n",
      "Epoch 431/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 5197.4135 - val_loss: 3881.9147\n",
      "Epoch 432/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 5183.7052 - val_loss: 3869.8037\n",
      "Epoch 433/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 5185.9793 - val_loss: 3873.5847\n",
      "Epoch 434/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 5175.8970 - val_loss: 3867.3481\n",
      "Epoch 435/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 5171.4199 - val_loss: 3865.1522\n",
      "Epoch 436/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 5165.1183 - val_loss: 3860.4674\n",
      "Epoch 437/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 5174.2962 - val_loss: 3863.6051\n",
      "Epoch 438/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 5162.7562 - val_loss: 3861.3317\n",
      "Epoch 439/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 5164.9937 - val_loss: 3858.8870\n",
      "Epoch 440/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 5154.9926 - val_loss: 3851.2768\n",
      "Epoch 441/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 5154.3335 - val_loss: 3871.2102\n",
      "Epoch 442/1000\n",
      "7438/7438 [==============================] - 1s 120us/step - loss: 5164.8593 - val_loss: 3890.3658\n",
      "Epoch 443/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 5158.8907 - val_loss: 3848.0272\n",
      "Epoch 444/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 5142.7352 - val_loss: 3846.1426\n",
      "Epoch 445/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 5133.6901 - val_loss: 3846.5219\n",
      "Epoch 446/1000\n",
      "7438/7438 [==============================] - 1s 122us/step - loss: 5147.0212 - val_loss: 3852.1741\n",
      "Epoch 447/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 5140.3281 - val_loss: 3845.7477\n",
      "Epoch 448/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 5127.5652 - val_loss: 3836.6160\n",
      "Epoch 449/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 5122.9350 - val_loss: 3835.7801\n",
      "Epoch 450/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 5118.7248 - val_loss: 3833.4574\n",
      "Epoch 451/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 5114.5664 - val_loss: 3839.6399\n",
      "Epoch 452/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 5116.3690 - val_loss: 3826.5685\n",
      "Epoch 453/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 5110.6784 - val_loss: 3824.1205\n",
      "Epoch 454/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 5107.2902 - val_loss: 3835.7702\n",
      "Epoch 455/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 5109.2990 - val_loss: 3829.4849\n",
      "Epoch 456/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 5100.4123 - val_loss: 3820.0830\n",
      "Epoch 457/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 5094.3376 - val_loss: 3814.5310\n",
      "Epoch 458/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 5101.1395 - val_loss: 3814.9299\n",
      "Epoch 459/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 5087.9117 - val_loss: 3813.1465\n",
      "Epoch 460/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 5083.5668 - val_loss: 3809.1820\n",
      "Epoch 461/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 5078.7005 - val_loss: 3813.2902\n",
      "Epoch 462/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 5084.0321 - val_loss: 3806.2375\n",
      "Epoch 463/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 5085.0764 - val_loss: 3806.5306\n",
      "Epoch 464/1000\n",
      "7438/7438 [==============================] - 1s 122us/step - loss: 5081.1288 - val_loss: 3816.1974\n",
      "Epoch 465/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 5073.8809 - val_loss: 3796.5906\n",
      "Epoch 466/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 5090.8975 - val_loss: 3819.2836\n",
      "Epoch 467/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 5072.3511 - val_loss: 3799.2922\n",
      "Epoch 468/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 5057.9386 - val_loss: 3814.6399\n",
      "Epoch 469/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 5081.3821 - val_loss: 3793.9011\n",
      "Epoch 470/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 5049.8359 - val_loss: 3792.2011\n",
      "Epoch 471/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 5048.8583 - val_loss: 3787.8199\n",
      "Epoch 472/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 5048.4353 - val_loss: 3789.4002\n",
      "Epoch 473/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 5043.0085 - val_loss: 3782.8305\n",
      "Epoch 474/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 5044.2662 - val_loss: 3776.2010\n",
      "Epoch 475/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 5036.1392 - val_loss: 3779.6829\n",
      "Epoch 476/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 5027.6901 - val_loss: 3774.4816\n",
      "Epoch 477/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 5029.3226 - val_loss: 3794.4925\n",
      "Epoch 478/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 5031.5872 - val_loss: 3779.5448\n",
      "Epoch 479/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 5020.5486 - val_loss: 3778.5339\n",
      "Epoch 480/1000\n",
      "7438/7438 [==============================] - 1s 122us/step - loss: 5018.7431 - val_loss: 3767.4872\n",
      "Epoch 481/1000\n",
      "7438/7438 [==============================] - 1s 121us/step - loss: 5011.5755 - val_loss: 3764.4649\n",
      "Epoch 482/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 5016.7562 - val_loss: 3767.1992\n",
      "Epoch 483/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 5013.1131 - val_loss: 3784.5509\n",
      "Epoch 484/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 5007.2485 - val_loss: 3762.7062\n",
      "Epoch 485/1000\n",
      "7438/7438 [==============================] - 1s 117us/step - loss: 4999.0747 - val_loss: 3759.9068\n",
      "Epoch 486/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 4997.1579 - val_loss: 3762.8595\n",
      "Epoch 487/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 4991.9582 - val_loss: 3770.5895\n",
      "Epoch 488/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 4989.0163 - val_loss: 3750.2457\n",
      "Epoch 489/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 4989.4547 - val_loss: 3748.7209\n",
      "Epoch 490/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 4984.1369 - val_loss: 3746.7114\n",
      "Epoch 491/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 4986.4232 - val_loss: 3747.1741\n",
      "Epoch 492/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 4981.5921 - val_loss: 3742.1755\n",
      "Epoch 493/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 4978.9372 - val_loss: 3741.2648\n",
      "Epoch 494/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 4969.6142 - val_loss: 3738.4692\n",
      "Epoch 495/1000\n",
      "7438/7438 [==============================] - 1s 131us/step - loss: 4962.7175 - val_loss: 3738.2401\n",
      "Epoch 496/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 4960.0017 - val_loss: 3735.1925\n",
      "Epoch 497/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 4957.1911 - val_loss: 3728.7883\n",
      "Epoch 498/1000\n",
      "7438/7438 [==============================] - 1s 132us/step - loss: 4958.8650 - val_loss: 3744.5748\n",
      "Epoch 499/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 4953.5377 - val_loss: 3738.0170\n",
      "Epoch 500/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 4953.4767 - val_loss: 3726.6112\n",
      "Epoch 501/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 4952.9194 - val_loss: 3737.4425\n",
      "Epoch 502/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 4938.6351 - val_loss: 3717.1824\n",
      "Epoch 503/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 4944.6412 - val_loss: 3723.1954\n",
      "Epoch 504/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 4965.2118 - val_loss: 3764.5085\n",
      "Epoch 505/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 4939.8538 - val_loss: 3713.4372\n",
      "Epoch 506/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 4923.2043 - val_loss: 3711.0613\n",
      "Epoch 507/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 4923.3125 - val_loss: 3708.4347\n",
      "Epoch 508/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 4917.6375 - val_loss: 3707.4087\n",
      "Epoch 509/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 4912.0298 - val_loss: 3701.4367\n",
      "Epoch 510/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 4910.5726 - val_loss: 3710.7262\n",
      "Epoch 511/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 4925.9066 - val_loss: 3700.5859\n",
      "Epoch 512/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 4914.0129 - val_loss: 3702.6408\n",
      "Epoch 513/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 4903.8189 - val_loss: 3699.6361\n",
      "Epoch 514/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 4898.3375 - val_loss: 3693.7335\n",
      "Epoch 515/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 4904.9362 - val_loss: 3711.5589\n",
      "Epoch 516/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 4896.6855 - val_loss: 3684.8014\n",
      "Epoch 517/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 4892.8289 - val_loss: 3686.6247\n",
      "Epoch 518/1000\n",
      "7438/7438 [==============================] - 1s 133us/step - loss: 4885.8667 - val_loss: 3689.4018\n",
      "Epoch 519/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 4893.0632 - val_loss: 3690.3480\n",
      "Epoch 520/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 4877.8308 - val_loss: 3686.2496\n",
      "Epoch 521/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 4875.8203 - val_loss: 3682.7114\n",
      "Epoch 522/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 4870.4282 - val_loss: 3678.5953\n",
      "Epoch 523/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 4867.4473 - val_loss: 3693.6306\n",
      "Epoch 524/1000\n",
      "7438/7438 [==============================] - 1s 132us/step - loss: 4869.2879 - val_loss: 3677.1036\n",
      "Epoch 525/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 4859.6629 - val_loss: 3673.8005\n",
      "Epoch 526/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 4855.8064 - val_loss: 3671.7358\n",
      "Epoch 527/1000\n",
      "7438/7438 [==============================] - 1s 119us/step - loss: 4861.2285 - val_loss: 3667.8350\n",
      "Epoch 528/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 4850.4766 - val_loss: 3663.8230\n",
      "Epoch 529/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 4846.7630 - val_loss: 3661.0319\n",
      "Epoch 530/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 4843.8133 - val_loss: 3674.1449\n",
      "Epoch 531/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 4857.0246 - val_loss: 3664.6281\n",
      "Epoch 532/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 4839.2867 - val_loss: 3661.8783\n",
      "Epoch 533/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 4836.0647 - val_loss: 3655.9931\n",
      "Epoch 534/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 4833.2540 - val_loss: 3655.3137\n",
      "Epoch 535/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 4824.6140 - val_loss: 3653.6797\n",
      "Epoch 536/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 4818.9199 - val_loss: 3676.0729\n",
      "Epoch 537/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 4830.0020 - val_loss: 3646.3233\n",
      "Epoch 538/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 4819.5016 - val_loss: 3641.9480\n",
      "Epoch 539/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 4824.3959 - val_loss: 3636.9509\n",
      "Epoch 540/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 4815.2682 - val_loss: 3637.1048\n",
      "Epoch 541/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 4812.8240 - val_loss: 3638.1770\n",
      "Epoch 542/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 4800.3283 - val_loss: 3633.5434\n",
      "Epoch 543/1000\n",
      "7438/7438 [==============================] - 1s 121us/step - loss: 4793.4004 - val_loss: 3626.3613\n",
      "Epoch 544/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 4805.1675 - val_loss: 3629.9296\n",
      "Epoch 545/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 4796.0297 - val_loss: 3636.6466\n",
      "Epoch 546/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 4799.7343 - val_loss: 3630.1970\n",
      "Epoch 547/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 4793.9777 - val_loss: 3624.3196\n",
      "Epoch 548/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 4778.2866 - val_loss: 3621.9584\n",
      "Epoch 549/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 4778.5589 - val_loss: 3621.5330\n",
      "Epoch 550/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 4785.6715 - val_loss: 3657.9532\n",
      "Epoch 551/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 4781.0219 - val_loss: 3617.1094\n",
      "Epoch 552/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 4766.6195 - val_loss: 3615.2381\n",
      "Epoch 553/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 4761.5675 - val_loss: 3612.7799\n",
      "Epoch 554/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 4762.7281 - val_loss: 3610.6170\n",
      "Epoch 555/1000\n",
      "7438/7438 [==============================] - 1s 122us/step - loss: 4758.7140 - val_loss: 3609.6514\n",
      "Epoch 556/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 4752.2665 - val_loss: 3612.5229\n",
      "Epoch 557/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 4748.9069 - val_loss: 3615.9417\n",
      "Epoch 558/1000\n",
      "7438/7438 [==============================] - 1s 122us/step - loss: 4748.4871 - val_loss: 3597.5485\n",
      "Epoch 559/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 4742.6797 - val_loss: 3598.4551\n",
      "Epoch 560/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 4738.7469 - val_loss: 3597.3056\n",
      "Epoch 561/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 4736.1995 - val_loss: 3594.5066\n",
      "Epoch 562/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 4751.9338 - val_loss: 3594.6151\n",
      "Epoch 563/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 4729.6739 - val_loss: 3589.8015\n",
      "Epoch 564/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 4723.9142 - val_loss: 3587.7262\n",
      "Epoch 565/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 4723.3346 - val_loss: 3583.7462\n",
      "Epoch 566/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 4735.6179 - val_loss: 3591.1238\n",
      "Epoch 567/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 4720.5639 - val_loss: 3582.5054\n",
      "Epoch 568/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 4712.2123 - val_loss: 3576.0813\n",
      "Epoch 569/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 4716.4923 - val_loss: 3585.4260\n",
      "Epoch 570/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 4712.1313 - val_loss: 3587.3805\n",
      "Epoch 571/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 4699.8420 - val_loss: 3574.5113\n",
      "Epoch 572/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 4697.0485 - val_loss: 3571.7568\n",
      "Epoch 573/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 4710.5366 - val_loss: 3570.0274\n",
      "Epoch 574/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 4689.4264 - val_loss: 3563.6666\n",
      "Epoch 575/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 4689.1795 - val_loss: 3573.6774\n",
      "Epoch 576/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 4702.1129 - val_loss: 3579.8100\n",
      "Epoch 577/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 4682.7945 - val_loss: 3565.6046\n",
      "Epoch 578/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 4677.3817 - val_loss: 3559.8408\n",
      "Epoch 579/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 4673.2132 - val_loss: 3554.2116\n",
      "Epoch 580/1000\n",
      "7438/7438 [==============================] - 1s 122us/step - loss: 4673.1181 - val_loss: 3549.4133\n",
      "Epoch 581/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 4684.1100 - val_loss: 3559.1301\n",
      "Epoch 582/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 4662.0863 - val_loss: 3550.0428\n",
      "Epoch 583/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 4663.0494 - val_loss: 3561.2063\n",
      "Epoch 584/1000\n",
      "7438/7438 [==============================] - 1s 132us/step - loss: 4659.7429 - val_loss: 3554.2487\n",
      "Epoch 585/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 4657.3075 - val_loss: 3558.4790\n",
      "Epoch 586/1000\n",
      "7438/7438 [==============================] - 1s 120us/step - loss: 4654.6801 - val_loss: 3550.9406\n",
      "Epoch 587/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 4653.1735 - val_loss: 3546.2246\n",
      "Epoch 588/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 4647.5396 - val_loss: 3543.7039\n",
      "Epoch 589/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 4663.4582 - val_loss: 3573.5212\n",
      "Epoch 590/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 4641.2633 - val_loss: 3533.0434\n",
      "Epoch 591/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 4650.7626 - val_loss: 3538.3941\n",
      "Epoch 592/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 4630.9904 - val_loss: 3535.4986\n",
      "Epoch 593/1000\n",
      "7438/7438 [==============================] - 1s 131us/step - loss: 4623.4747 - val_loss: 3528.8353\n",
      "Epoch 594/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 4645.0727 - val_loss: 3536.9646\n",
      "Epoch 595/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 4627.6118 - val_loss: 3529.3432\n",
      "Epoch 596/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 4622.7569 - val_loss: 3563.8983\n",
      "Epoch 597/1000\n",
      "7438/7438 [==============================] - 1s 132us/step - loss: 4625.3585 - val_loss: 3518.5589\n",
      "Epoch 598/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 4626.7284 - val_loss: 3531.5152\n",
      "Epoch 599/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 4661.9754 - val_loss: 3555.7512\n",
      "Epoch 600/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 4634.9459 - val_loss: 3529.4774\n",
      "Epoch 601/1000\n",
      "7438/7438 [==============================] - 1s 121us/step - loss: 4624.1081 - val_loss: 3513.3658\n",
      "Epoch 602/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 4619.0495 - val_loss: 3522.8894\n",
      "Epoch 603/1000\n",
      "7438/7438 [==============================] - 1s 120us/step - loss: 4611.8879 - val_loss: 3508.2887\n",
      "Epoch 604/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 4621.2949 - val_loss: 3512.9350\n",
      "Epoch 605/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 4598.2103 - val_loss: 3507.6708\n",
      "Epoch 606/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 4592.4399 - val_loss: 3497.6362\n",
      "Epoch 607/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 4604.5941 - val_loss: 3509.4608\n",
      "Epoch 608/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 4593.6936 - val_loss: 3496.1734\n",
      "Epoch 609/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 4586.0692 - val_loss: 3496.9421\n",
      "Epoch 610/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 4584.5559 - val_loss: 3493.7721\n",
      "Epoch 611/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 4581.6606 - val_loss: 3501.6604\n",
      "Epoch 612/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 4580.3279 - val_loss: 3492.5591\n",
      "Epoch 613/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 4570.3010 - val_loss: 3491.1943\n",
      "Epoch 614/1000\n",
      "7438/7438 [==============================] - 1s 131us/step - loss: 4566.2792 - val_loss: 3487.5835\n",
      "Epoch 615/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 4564.2049 - val_loss: 3487.4510\n",
      "Epoch 616/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 4558.8345 - val_loss: 3485.9924\n",
      "Epoch 617/1000\n",
      "7438/7438 [==============================] - 1s 122us/step - loss: 4554.3515 - val_loss: 3475.2165\n",
      "Epoch 618/1000\n",
      "7438/7438 [==============================] - 1s 122us/step - loss: 4577.1680 - val_loss: 3480.0618\n",
      "Epoch 619/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 4550.9607 - val_loss: 3474.3835\n",
      "Epoch 620/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 4632.4728 - val_loss: 3552.2978\n",
      "Epoch 621/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 4578.1634 - val_loss: 3476.5611\n",
      "Epoch 622/1000\n",
      "7438/7438 [==============================] - 1s 122us/step - loss: 4543.4325 - val_loss: 3477.6656\n",
      "Epoch 623/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 4538.4420 - val_loss: 3464.3226\n",
      "Epoch 624/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 4539.5420 - val_loss: 3462.1625\n",
      "Epoch 625/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 4538.7039 - val_loss: 3494.5740\n",
      "Epoch 626/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 4536.6464 - val_loss: 3461.0621\n",
      "Epoch 627/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 4523.8610 - val_loss: 3465.6374\n",
      "Epoch 628/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 4520.7390 - val_loss: 3456.1855\n",
      "Epoch 629/1000\n",
      "7438/7438 [==============================] - 1s 132us/step - loss: 4514.4304 - val_loss: 3453.5014\n",
      "Epoch 630/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 4511.3692 - val_loss: 3447.6712\n",
      "Epoch 631/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 4511.1091 - val_loss: 3447.5834\n",
      "Epoch 632/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 4504.9462 - val_loss: 3444.7591\n",
      "Epoch 633/1000\n",
      "7438/7438 [==============================] - 1s 121us/step - loss: 4516.4639 - val_loss: 3468.6769\n",
      "Epoch 634/1000\n",
      "7438/7438 [==============================] - 1s 121us/step - loss: 4505.5835 - val_loss: 3442.7326\n",
      "Epoch 635/1000\n",
      "7438/7438 [==============================] - 1s 121us/step - loss: 4494.0998 - val_loss: 3440.8646\n",
      "Epoch 636/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 4493.0827 - val_loss: 3442.1662\n",
      "Epoch 637/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 4488.5888 - val_loss: 3439.8945\n",
      "Epoch 638/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 4484.0422 - val_loss: 3435.7630\n",
      "Epoch 639/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 4482.4682 - val_loss: 3429.5765\n",
      "Epoch 640/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 4478.2004 - val_loss: 3430.5739\n",
      "Epoch 641/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 4477.6208 - val_loss: 3436.4059\n",
      "Epoch 642/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 4472.5109 - val_loss: 3423.5227\n",
      "Epoch 643/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 4467.9091 - val_loss: 3422.4209\n",
      "Epoch 644/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 4468.4928 - val_loss: 3431.4468\n",
      "Epoch 645/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 4463.0694 - val_loss: 3419.7183\n",
      "Epoch 646/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 4463.5206 - val_loss: 3442.4186\n",
      "Epoch 647/1000\n",
      "7438/7438 [==============================] - 1s 132us/step - loss: 4459.4730 - val_loss: 3413.4902\n",
      "Epoch 648/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 4457.7253 - val_loss: 3430.5356\n",
      "Epoch 649/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 4452.3776 - val_loss: 3414.2130\n",
      "Epoch 650/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 4446.6595 - val_loss: 3412.4545\n",
      "Epoch 651/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 4447.7939 - val_loss: 3410.6424\n",
      "Epoch 652/1000\n",
      "7438/7438 [==============================] - 1s 131us/step - loss: 4466.4738 - val_loss: 3424.9494\n",
      "Epoch 653/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 4439.1053 - val_loss: 3408.2102\n",
      "Epoch 654/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 4431.7908 - val_loss: 3404.0718\n",
      "Epoch 655/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 4429.7457 - val_loss: 3398.9728\n",
      "Epoch 656/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 4434.0578 - val_loss: 3409.7222\n",
      "Epoch 657/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 4438.8430 - val_loss: 3412.5011\n",
      "Epoch 658/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 4424.2212 - val_loss: 3403.6311\n",
      "Epoch 659/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 4416.5667 - val_loss: 3395.0382\n",
      "Epoch 660/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 4422.3040 - val_loss: 3392.2476\n",
      "Epoch 661/1000\n",
      "7438/7438 [==============================] - 1s 131us/step - loss: 4409.4717 - val_loss: 3391.9379\n",
      "Epoch 662/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 4404.7267 - val_loss: 3387.9331\n",
      "Epoch 663/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 4406.9494 - val_loss: 3414.3100\n",
      "Epoch 664/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 4408.9890 - val_loss: 3384.7040\n",
      "Epoch 665/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 4397.8777 - val_loss: 3384.2288\n",
      "Epoch 666/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 4401.7590 - val_loss: 3401.8321\n",
      "Epoch 667/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 4394.7437 - val_loss: 3382.0120\n",
      "Epoch 668/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 4388.8763 - val_loss: 3380.4846\n",
      "Epoch 669/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 4384.7784 - val_loss: 3382.4564\n",
      "Epoch 670/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 4383.0985 - val_loss: 3374.3976\n",
      "Epoch 671/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 4377.0338 - val_loss: 3370.9636\n",
      "Epoch 672/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 4377.0993 - val_loss: 3361.3023\n",
      "Epoch 673/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 4407.0738 - val_loss: 3409.1918\n",
      "Epoch 674/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 4377.0307 - val_loss: 3366.1318\n",
      "Epoch 675/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 4364.1627 - val_loss: 3360.4706\n",
      "Epoch 676/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 4366.0886 - val_loss: 3375.6615\n",
      "Epoch 677/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 4370.9514 - val_loss: 3360.4205\n",
      "Epoch 678/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 4359.0699 - val_loss: 3364.8999\n",
      "Epoch 679/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 4355.6019 - val_loss: 3358.6560\n",
      "Epoch 680/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 4352.3248 - val_loss: 3353.5834\n",
      "Epoch 681/1000\n",
      "7438/7438 [==============================] - 1s 120us/step - loss: 4350.4853 - val_loss: 3372.5211\n",
      "Epoch 682/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 4352.8839 - val_loss: 3351.9140\n",
      "Epoch 683/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 4347.1654 - val_loss: 3350.6750\n",
      "Epoch 684/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 4345.4333 - val_loss: 3348.3688\n",
      "Epoch 685/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 4339.5149 - val_loss: 3340.4422\n",
      "Epoch 686/1000\n",
      "7438/7438 [==============================] - 1s 121us/step - loss: 4339.2571 - val_loss: 3345.6046\n",
      "Epoch 687/1000\n",
      "7438/7438 [==============================] - 1s 118us/step - loss: 4335.6888 - val_loss: 3343.4792\n",
      "Epoch 688/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 4330.7068 - val_loss: 3340.9091\n",
      "Epoch 689/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 4330.3958 - val_loss: 3348.6538\n",
      "Epoch 690/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 4326.6457 - val_loss: 3330.6490\n",
      "Epoch 691/1000\n",
      "7438/7438 [==============================] - 1s 121us/step - loss: 4323.4301 - val_loss: 3355.3234\n",
      "Epoch 692/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 4323.4855 - val_loss: 3329.0712\n",
      "Epoch 693/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 4337.8803 - val_loss: 3347.2766\n",
      "Epoch 694/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 4311.6153 - val_loss: 3322.4158\n",
      "Epoch 695/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 4306.6459 - val_loss: 3323.0297\n",
      "Epoch 696/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 4305.0673 - val_loss: 3332.5351\n",
      "Epoch 697/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 4303.4349 - val_loss: 3316.4897\n",
      "Epoch 698/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 4300.7159 - val_loss: 3315.3406\n",
      "Epoch 699/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 4298.1271 - val_loss: 3316.5715\n",
      "Epoch 700/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 4296.2445 - val_loss: 3311.6630\n",
      "Epoch 701/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 4291.4270 - val_loss: 3318.0801\n",
      "Epoch 702/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 4296.3314 - val_loss: 3309.6929\n",
      "Epoch 703/1000\n",
      "7438/7438 [==============================] - 1s 119us/step - loss: 4295.0144 - val_loss: 3312.0544\n",
      "Epoch 704/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 4283.9571 - val_loss: 3309.3817\n",
      "Epoch 705/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 4289.8847 - val_loss: 3327.4381\n",
      "Epoch 706/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 4278.5423 - val_loss: 3301.9506\n",
      "Epoch 707/1000\n",
      "7438/7438 [==============================] - 1s 121us/step - loss: 4271.9525 - val_loss: 3297.6044\n",
      "Epoch 708/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 4270.6999 - val_loss: 3298.0962\n",
      "Epoch 709/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 4265.9766 - val_loss: 3295.5875\n",
      "Epoch 710/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 4262.3274 - val_loss: 3290.6078\n",
      "Epoch 711/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 4261.6786 - val_loss: 3287.3558\n",
      "Epoch 712/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 4259.0657 - val_loss: 3285.5658\n",
      "Epoch 713/1000\n",
      "7438/7438 [==============================] - 1s 131us/step - loss: 4257.8984 - val_loss: 3326.8954\n",
      "Epoch 714/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 4262.8133 - val_loss: 3279.3354\n",
      "Epoch 715/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 4248.2223 - val_loss: 3278.3508\n",
      "Epoch 716/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 4246.6440 - val_loss: 3279.6838\n",
      "Epoch 717/1000\n",
      "7438/7438 [==============================] - 1s 122us/step - loss: 4240.9481 - val_loss: 3276.2814\n",
      "Epoch 718/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 4247.4989 - val_loss: 3278.2175\n",
      "Epoch 719/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 4237.4167 - val_loss: 3282.8915\n",
      "Epoch 720/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 4234.9111 - val_loss: 3267.3383\n",
      "Epoch 721/1000\n",
      "7438/7438 [==============================] - 1s 131us/step - loss: 4228.8548 - val_loss: 3267.1247\n",
      "Epoch 722/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 4226.0060 - val_loss: 3268.0007\n",
      "Epoch 723/1000\n",
      "7438/7438 [==============================] - 1s 122us/step - loss: 4233.4944 - val_loss: 3293.9757\n",
      "Epoch 724/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 4223.3940 - val_loss: 3262.0079\n",
      "Epoch 725/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 4228.3043 - val_loss: 3256.0947\n",
      "Epoch 726/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 4216.3372 - val_loss: 3262.8193\n",
      "Epoch 727/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 4211.5260 - val_loss: 3259.2527\n",
      "Epoch 728/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 4211.1561 - val_loss: 3253.2207\n",
      "Epoch 729/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 4211.3873 - val_loss: 3266.2073\n",
      "Epoch 730/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 4226.2126 - val_loss: 3252.6840\n",
      "Epoch 731/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 4205.7245 - val_loss: 3255.7705\n",
      "Epoch 732/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 4195.5780 - val_loss: 3245.3003\n",
      "Epoch 733/1000\n",
      "7438/7438 [==============================] - 1s 122us/step - loss: 4191.9256 - val_loss: 3240.6514\n",
      "Epoch 734/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 4193.6046 - val_loss: 3245.0604\n",
      "Epoch 735/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 4195.8226 - val_loss: 3250.3887\n",
      "Epoch 736/1000\n",
      "7438/7438 [==============================] - 1s 131us/step - loss: 4185.2413 - val_loss: 3240.0241\n",
      "Epoch 737/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 4180.5802 - val_loss: 3235.7077\n",
      "Epoch 738/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 4177.6347 - val_loss: 3248.0439\n",
      "Epoch 739/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 4173.9350 - val_loss: 3232.4849\n",
      "Epoch 740/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 4175.7439 - val_loss: 3231.9744\n",
      "Epoch 741/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 4168.6625 - val_loss: 3227.8180\n",
      "Epoch 742/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 4180.7030 - val_loss: 3233.6725\n",
      "Epoch 743/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 4175.0582 - val_loss: 3249.6428\n",
      "Epoch 744/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 4158.3206 - val_loss: 3221.7796\n",
      "Epoch 745/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 4172.5834 - val_loss: 3231.6865\n",
      "Epoch 746/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 4157.5097 - val_loss: 3221.0602\n",
      "Epoch 747/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 4153.5090 - val_loss: 3224.5848\n",
      "Epoch 748/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 4149.5335 - val_loss: 3229.8008\n",
      "Epoch 749/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 4150.1613 - val_loss: 3233.8301\n",
      "Epoch 750/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 4146.2052 - val_loss: 3213.4583\n",
      "Epoch 751/1000\n",
      "7438/7438 [==============================] - 1s 133us/step - loss: 4139.0047 - val_loss: 3217.4346\n",
      "Epoch 752/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 4136.9360 - val_loss: 3208.9487\n",
      "Epoch 753/1000\n",
      "7438/7438 [==============================] - 1s 131us/step - loss: 4136.7525 - val_loss: 3220.1921\n",
      "Epoch 754/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 4132.7337 - val_loss: 3199.9258\n",
      "Epoch 755/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 4137.4808 - val_loss: 3257.5057\n",
      "Epoch 756/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 4132.6687 - val_loss: 3199.3458\n",
      "Epoch 757/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 4129.2776 - val_loss: 3197.7907\n",
      "Epoch 758/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 4147.6489 - val_loss: 3238.6452\n",
      "Epoch 759/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 4119.8801 - val_loss: 3195.7651\n",
      "Epoch 760/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 4111.6942 - val_loss: 3190.5415\n",
      "Epoch 761/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 4112.5163 - val_loss: 3192.1750\n",
      "Epoch 762/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 4109.1698 - val_loss: 3192.8137\n",
      "Epoch 763/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 4103.3957 - val_loss: 3187.7178\n",
      "Epoch 764/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 4100.7289 - val_loss: 3192.1863\n",
      "Epoch 765/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 4123.2605 - val_loss: 3187.9646\n",
      "Epoch 766/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 4094.0405 - val_loss: 3186.5545\n",
      "Epoch 767/1000\n",
      "7438/7438 [==============================] - 1s 132us/step - loss: 4093.6205 - val_loss: 3184.7587\n",
      "Epoch 768/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 4088.7432 - val_loss: 3179.6807\n",
      "Epoch 769/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 4083.7481 - val_loss: 3176.0636\n",
      "Epoch 770/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 4087.6239 - val_loss: 3175.4876\n",
      "Epoch 771/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 4078.1310 - val_loss: 3173.4461\n",
      "Epoch 772/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 4082.9750 - val_loss: 3203.1976\n",
      "Epoch 773/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 4087.9126 - val_loss: 3206.5235\n",
      "Epoch 774/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 4086.1887 - val_loss: 3214.1548\n",
      "Epoch 775/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 4061.8718 - val_loss: 3192.2157\n",
      "Epoch 776/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 4061.2555 - val_loss: 3184.5548\n",
      "Epoch 777/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 4051.8947 - val_loss: 3186.7780\n",
      "Epoch 778/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 4061.9166 - val_loss: 3210.2344\n",
      "Epoch 779/1000\n",
      "7438/7438 [==============================] - 1s 122us/step - loss: 4044.5740 - val_loss: 3170.9799\n",
      "Epoch 780/1000\n",
      "7438/7438 [==============================] - 1s 131us/step - loss: 4043.8821 - val_loss: 3171.4567\n",
      "Epoch 781/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 4023.9867 - val_loss: 3166.8135\n",
      "Epoch 782/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 4020.4406 - val_loss: 3164.1546\n",
      "Epoch 783/1000\n",
      "7438/7438 [==============================] - 1s 120us/step - loss: 4011.7782 - val_loss: 3163.9089\n",
      "Epoch 784/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 4010.2007 - val_loss: 3172.4615\n",
      "Epoch 785/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 4020.2188 - val_loss: 3178.1018\n",
      "Epoch 786/1000\n",
      "7438/7438 [==============================] - 1s 122us/step - loss: 4001.8054 - val_loss: 3162.4661\n",
      "Epoch 787/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 4006.1494 - val_loss: 3163.8110\n",
      "Epoch 788/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 3996.0238 - val_loss: 3156.8404\n",
      "Epoch 789/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 3991.4904 - val_loss: 3170.6766\n",
      "Epoch 790/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 3993.6676 - val_loss: 3169.5375\n",
      "Epoch 791/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 3986.8052 - val_loss: 3152.6663\n",
      "Epoch 792/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 3982.1014 - val_loss: 3155.1990\n",
      "Epoch 793/1000\n",
      "7438/7438 [==============================] - 1s 131us/step - loss: 3983.5867 - val_loss: 3162.4473\n",
      "Epoch 794/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 3980.6905 - val_loss: 3148.8863\n",
      "Epoch 795/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 3978.1684 - val_loss: 3154.7065\n",
      "Epoch 796/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 3967.7094 - val_loss: 3164.7184\n",
      "Epoch 797/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 3967.1887 - val_loss: 3149.1018\n",
      "Epoch 798/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 3985.1523 - val_loss: 3166.2314\n",
      "Epoch 799/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 3964.7745 - val_loss: 3146.2160\n",
      "Epoch 800/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 3965.1685 - val_loss: 3147.0111\n",
      "Epoch 801/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 3979.3464 - val_loss: 3149.3642\n",
      "Epoch 802/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 3969.4401 - val_loss: 3133.0595\n",
      "Epoch 803/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 3953.7531 - val_loss: 3142.9541\n",
      "Epoch 804/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 3954.2378 - val_loss: 3149.4922\n",
      "Epoch 805/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 3944.3328 - val_loss: 3135.6548\n",
      "Epoch 806/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 3940.3333 - val_loss: 3133.2046\n",
      "Epoch 807/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 3937.0892 - val_loss: 3127.8007\n",
      "Epoch 808/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 3942.2594 - val_loss: 3150.0378\n",
      "Epoch 809/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 3933.5789 - val_loss: 3127.5465\n",
      "Epoch 810/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 3926.1765 - val_loss: 3128.9820\n",
      "Epoch 811/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 3924.7830 - val_loss: 3125.2626\n",
      "Epoch 812/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 3936.3893 - val_loss: 3114.5773\n",
      "Epoch 813/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 3920.3575 - val_loss: 3120.8831\n",
      "Epoch 814/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 3916.7490 - val_loss: 3130.3209\n",
      "Epoch 815/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 3911.8224 - val_loss: 3119.6075\n",
      "Epoch 816/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 3922.1225 - val_loss: 3180.1903\n",
      "Epoch 817/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 3933.6819 - val_loss: 3112.3868\n",
      "Epoch 818/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 3907.2050 - val_loss: 3111.8298\n",
      "Epoch 819/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 3907.5759 - val_loss: 3117.9478\n",
      "Epoch 820/1000\n",
      "7438/7438 [==============================] - 1s 122us/step - loss: 3900.9485 - val_loss: 3108.1630\n",
      "Epoch 821/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 3897.2446 - val_loss: 3159.6985\n",
      "Epoch 822/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 3977.9232 - val_loss: 3143.0586\n",
      "Epoch 823/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 3900.4197 - val_loss: 3115.4288\n",
      "Epoch 824/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 3891.6363 - val_loss: 3100.2543\n",
      "Epoch 825/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 3892.1062 - val_loss: 3106.8728\n",
      "Epoch 826/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 3884.8478 - val_loss: 3105.7963\n",
      "Epoch 827/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 3879.1475 - val_loss: 3100.3078\n",
      "Epoch 828/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 3881.5663 - val_loss: 3095.4557\n",
      "Epoch 829/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 3873.4549 - val_loss: 3096.7721\n",
      "Epoch 830/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 3869.0661 - val_loss: 3102.1449\n",
      "Epoch 831/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 3868.0864 - val_loss: 3090.5408\n",
      "Epoch 832/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 3865.3610 - val_loss: 3087.8400\n",
      "Epoch 833/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 3883.6020 - val_loss: 3106.3115\n",
      "Epoch 834/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 3867.2127 - val_loss: 3108.6490\n",
      "Epoch 835/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 3862.6253 - val_loss: 3083.7219\n",
      "Epoch 836/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 3881.5224 - val_loss: 3086.7691\n",
      "Epoch 837/1000\n",
      "7438/7438 [==============================] - 1s 119us/step - loss: 3877.8093 - val_loss: 3093.0180\n",
      "Epoch 838/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 3851.4362 - val_loss: 3084.8538\n",
      "Epoch 839/1000\n",
      "7438/7438 [==============================] - 1s 122us/step - loss: 3846.4686 - val_loss: 3080.4821\n",
      "Epoch 840/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 3847.7891 - val_loss: 3083.7086\n",
      "Epoch 841/1000\n",
      "7438/7438 [==============================] - 1s 121us/step - loss: 3850.1940 - val_loss: 3081.5155\n",
      "Epoch 842/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 3837.0977 - val_loss: 3087.2438\n",
      "Epoch 843/1000\n",
      "7438/7438 [==============================] - 1s 120us/step - loss: 3834.8366 - val_loss: 3085.4646\n",
      "Epoch 844/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 3836.7139 - val_loss: 3070.4673\n",
      "Epoch 845/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 3830.2895 - val_loss: 3081.4300\n",
      "Epoch 846/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 3826.1754 - val_loss: 3063.8814\n",
      "Epoch 847/1000\n",
      "7438/7438 [==============================] - 1s 122us/step - loss: 3821.9823 - val_loss: 3063.7822\n",
      "Epoch 848/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 3821.7204 - val_loss: 3066.2859\n",
      "Epoch 849/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 3818.4054 - val_loss: 3064.5768\n",
      "Epoch 850/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 3817.6622 - val_loss: 3062.3758\n",
      "Epoch 851/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 3809.6992 - val_loss: 3051.8674\n",
      "Epoch 852/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 3829.8077 - val_loss: 3060.3046\n",
      "Epoch 853/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 3805.5678 - val_loss: 3053.5658\n",
      "Epoch 854/1000\n",
      "7438/7438 [==============================] - 1s 122us/step - loss: 3812.0985 - val_loss: 3056.2855\n",
      "Epoch 855/1000\n",
      "7438/7438 [==============================] - 1s 121us/step - loss: 3801.1521 - val_loss: 3052.4356\n",
      "Epoch 856/1000\n",
      "7438/7438 [==============================] - 1s 121us/step - loss: 3808.0098 - val_loss: 3051.5183\n",
      "Epoch 857/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 3798.5842 - val_loss: 3047.0880\n",
      "Epoch 858/1000\n",
      "7438/7438 [==============================] - 1s 131us/step - loss: 3796.3879 - val_loss: 3045.0627\n",
      "Epoch 859/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 3792.1883 - val_loss: 3044.2879\n",
      "Epoch 860/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 3788.9067 - val_loss: 3038.9952\n",
      "Epoch 861/1000\n",
      "7438/7438 [==============================] - 1s 122us/step - loss: 3808.0127 - val_loss: 3089.2386\n",
      "Epoch 862/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 3798.1311 - val_loss: 3043.6025\n",
      "Epoch 863/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 3782.0214 - val_loss: 3035.8656\n",
      "Epoch 864/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 3780.8947 - val_loss: 3040.9178\n",
      "Epoch 865/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 3777.8891 - val_loss: 3033.9598\n",
      "Epoch 866/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 3774.4133 - val_loss: 3030.3814\n",
      "Epoch 867/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 3766.8179 - val_loss: 3035.2116\n",
      "Epoch 868/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 3768.0850 - val_loss: 3027.6446\n",
      "Epoch 869/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 3766.0191 - val_loss: 3032.9363\n",
      "Epoch 870/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 3774.4206 - val_loss: 3050.3010\n",
      "Epoch 871/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 3790.5072 - val_loss: 3025.5916\n",
      "Epoch 872/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 3759.7792 - val_loss: 3029.5498\n",
      "Epoch 873/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 3758.1272 - val_loss: 3031.9714\n",
      "Epoch 874/1000\n",
      "7438/7438 [==============================] - 1s 120us/step - loss: 3755.1805 - val_loss: 3023.3296\n",
      "Epoch 875/1000\n",
      "7438/7438 [==============================] - 1s 122us/step - loss: 3753.6202 - val_loss: 3033.7146\n",
      "Epoch 876/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 3749.1352 - val_loss: 3017.8670\n",
      "Epoch 877/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 3744.9996 - val_loss: 3025.4964\n",
      "Epoch 878/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 3752.9236 - val_loss: 3033.8469\n",
      "Epoch 879/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 3742.4003 - val_loss: 3021.7685\n",
      "Epoch 880/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 3738.1144 - val_loss: 3011.0137\n",
      "Epoch 881/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 3730.6321 - val_loss: 3010.1470\n",
      "Epoch 882/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 3732.4643 - val_loss: 3014.8421\n",
      "Epoch 883/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 3727.7474 - val_loss: 3003.4587\n",
      "Epoch 884/1000\n",
      "7438/7438 [==============================] - 1s 120us/step - loss: 3724.3388 - val_loss: 3005.5034\n",
      "Epoch 885/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 3721.8507 - val_loss: 3001.7107\n",
      "Epoch 886/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 3717.9035 - val_loss: 2999.7895\n",
      "Epoch 887/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 3714.4253 - val_loss: 3000.4784\n",
      "Epoch 888/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 3720.5745 - val_loss: 2999.8461\n",
      "Epoch 889/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 3712.8143 - val_loss: 3002.3555\n",
      "Epoch 890/1000\n",
      "7438/7438 [==============================] - 1s 122us/step - loss: 3713.4114 - val_loss: 3002.0353\n",
      "Epoch 891/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 3715.5599 - val_loss: 2995.2503\n",
      "Epoch 892/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 3747.1253 - val_loss: 2991.5106\n",
      "Epoch 893/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 3731.3276 - val_loss: 2996.7759\n",
      "Epoch 894/1000\n",
      "7438/7438 [==============================] - 1s 121us/step - loss: 3710.7394 - val_loss: 2999.8791\n",
      "Epoch 895/1000\n",
      "7438/7438 [==============================] - 1s 121us/step - loss: 3703.1097 - val_loss: 3014.0505\n",
      "Epoch 896/1000\n",
      "7438/7438 [==============================] - 1s 121us/step - loss: 3701.9904 - val_loss: 2991.2992\n",
      "Epoch 897/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 3690.8921 - val_loss: 2984.6299\n",
      "Epoch 898/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 3689.3616 - val_loss: 2980.5547\n",
      "Epoch 899/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 3684.3323 - val_loss: 2979.1210\n",
      "Epoch 900/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 3683.0775 - val_loss: 2983.2960\n",
      "Epoch 901/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 3680.5170 - val_loss: 2996.7563\n",
      "Epoch 902/1000\n",
      "7438/7438 [==============================] - 1s 133us/step - loss: 3684.9595 - val_loss: 2985.3720\n",
      "Epoch 903/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 3676.2970 - val_loss: 2976.1229\n",
      "Epoch 904/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 3671.7218 - val_loss: 2974.6894\n",
      "Epoch 905/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 3667.9974 - val_loss: 2979.9780\n",
      "Epoch 906/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 3671.8781 - val_loss: 2983.6064\n",
      "Epoch 907/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 3677.9739 - val_loss: 2989.3636\n",
      "Epoch 908/1000\n",
      "7438/7438 [==============================] - 1s 131us/step - loss: 3667.1995 - val_loss: 2968.5419\n",
      "Epoch 909/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 3658.8860 - val_loss: 2970.8633\n",
      "Epoch 910/1000\n",
      "7438/7438 [==============================] - 1s 133us/step - loss: 3660.2945 - val_loss: 2962.7798\n",
      "Epoch 911/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 3652.2410 - val_loss: 2968.9881\n",
      "Epoch 912/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 3650.9661 - val_loss: 2970.3076\n",
      "Epoch 913/1000\n",
      "7438/7438 [==============================] - 1s 131us/step - loss: 3649.8741 - val_loss: 2959.6317\n",
      "Epoch 914/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 3644.4850 - val_loss: 2963.8905\n",
      "Epoch 915/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 3640.8690 - val_loss: 2959.9835\n",
      "Epoch 916/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 3641.4243 - val_loss: 2960.9788\n",
      "Epoch 917/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 3640.0411 - val_loss: 2955.9537\n",
      "Epoch 918/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 3635.9984 - val_loss: 2948.3676\n",
      "Epoch 919/1000\n",
      "7438/7438 [==============================] - 1s 121us/step - loss: 3637.0754 - val_loss: 2949.9801\n",
      "Epoch 920/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 3630.5418 - val_loss: 2946.6676\n",
      "Epoch 921/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 3632.4909 - val_loss: 2948.0643\n",
      "Epoch 922/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 3633.3709 - val_loss: 2943.7662\n",
      "Epoch 923/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 3623.9024 - val_loss: 2947.3013\n",
      "Epoch 924/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 3623.0829 - val_loss: 2950.8453\n",
      "Epoch 925/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 3624.7847 - val_loss: 2948.7296\n",
      "Epoch 926/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 3627.8945 - val_loss: 2948.4287\n",
      "Epoch 927/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 3627.2927 - val_loss: 2950.7109\n",
      "Epoch 928/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 3613.6610 - val_loss: 2937.1893\n",
      "Epoch 929/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 3622.0420 - val_loss: 2947.8941\n",
      "Epoch 930/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 3617.4200 - val_loss: 2939.9740\n",
      "Epoch 931/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 3613.5317 - val_loss: 2938.0925\n",
      "Epoch 932/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 3618.5443 - val_loss: 2929.7648\n",
      "Epoch 933/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 3600.3607 - val_loss: 2931.3263\n",
      "Epoch 934/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 3598.3018 - val_loss: 2927.3233\n",
      "Epoch 935/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 3597.8167 - val_loss: 2937.1834\n",
      "Epoch 936/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 3589.6758 - val_loss: 2923.2751\n",
      "Epoch 937/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 3587.5513 - val_loss: 2929.7652\n",
      "Epoch 938/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 3586.0450 - val_loss: 2924.0557\n",
      "Epoch 939/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 3589.0466 - val_loss: 2911.4611\n",
      "Epoch 940/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 3597.1525 - val_loss: 2917.7896\n",
      "Epoch 941/1000\n",
      "7438/7438 [==============================] - 1s 122us/step - loss: 3584.5917 - val_loss: 2913.1220\n",
      "Epoch 942/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 3579.9149 - val_loss: 2921.3879\n",
      "Epoch 943/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 3574.6707 - val_loss: 2916.8475\n",
      "Epoch 944/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 3572.8039 - val_loss: 2921.6236\n",
      "Epoch 945/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 3571.5022 - val_loss: 2911.2708\n",
      "Epoch 946/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 3569.2336 - val_loss: 2914.4722\n",
      "Epoch 947/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 3564.7099 - val_loss: 2909.0235\n",
      "Epoch 948/1000\n",
      "7438/7438 [==============================] - 1s 121us/step - loss: 3560.5263 - val_loss: 2908.3538\n",
      "Epoch 949/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 3562.7682 - val_loss: 2901.5591\n",
      "Epoch 950/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 3565.1640 - val_loss: 2904.4273\n",
      "Epoch 951/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 3555.7448 - val_loss: 2908.2294\n",
      "Epoch 952/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 3549.5934 - val_loss: 2904.1156\n",
      "Epoch 953/1000\n",
      "7438/7438 [==============================] - 1s 121us/step - loss: 3546.7078 - val_loss: 2904.3069\n",
      "Epoch 954/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 3549.6419 - val_loss: 2893.4195\n",
      "Epoch 955/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 3544.9679 - val_loss: 2897.1793\n",
      "Epoch 956/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 3542.1039 - val_loss: 2891.2570\n",
      "Epoch 957/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 3544.1727 - val_loss: 2896.2436\n",
      "Epoch 958/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 3541.2380 - val_loss: 2896.6203\n",
      "Epoch 959/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 3540.2921 - val_loss: 2896.0993\n",
      "Epoch 960/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 3541.5212 - val_loss: 2896.7665\n",
      "Epoch 961/1000\n",
      "7438/7438 [==============================] - 1s 122us/step - loss: 3533.0629 - val_loss: 2891.9215\n",
      "Epoch 962/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 3530.8575 - val_loss: 2891.5046\n",
      "Epoch 963/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 3533.0604 - val_loss: 2884.8114\n",
      "Epoch 964/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 3527.8824 - val_loss: 2886.2588\n",
      "Epoch 965/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 3520.1434 - val_loss: 2878.5790\n",
      "Epoch 966/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 3526.3981 - val_loss: 2886.9714\n",
      "Epoch 967/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 3535.3002 - val_loss: 2879.4497\n",
      "Epoch 968/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 3511.5578 - val_loss: 2873.3809\n",
      "Epoch 969/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 3507.7504 - val_loss: 2882.9627\n",
      "Epoch 970/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 3508.2124 - val_loss: 2874.8536\n",
      "Epoch 971/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 3512.4981 - val_loss: 2897.7226\n",
      "Epoch 972/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 3525.3650 - val_loss: 2875.9655\n",
      "Epoch 973/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 3520.3886 - val_loss: 2880.0100\n",
      "Epoch 974/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 3503.6972 - val_loss: 2877.1214\n",
      "Epoch 975/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 3503.6256 - val_loss: 2873.4401\n",
      "Epoch 976/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 3494.7981 - val_loss: 2866.4218\n",
      "Epoch 977/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 3489.1521 - val_loss: 2864.7352\n",
      "Epoch 978/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 3501.8254 - val_loss: 2868.8280\n",
      "Epoch 979/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 3481.9868 - val_loss: 2909.4340\n",
      "Epoch 980/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 3503.0984 - val_loss: 2859.1185\n",
      "Epoch 981/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 3481.0057 - val_loss: 2861.3416\n",
      "Epoch 982/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 3477.3821 - val_loss: 2862.9760\n",
      "Epoch 983/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 3475.2892 - val_loss: 2858.6475\n",
      "Epoch 984/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 3474.7485 - val_loss: 2853.8941\n",
      "Epoch 985/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 3471.4716 - val_loss: 2854.0346\n",
      "Epoch 986/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 3467.2781 - val_loss: 2852.0085\n",
      "Epoch 987/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 3465.8322 - val_loss: 2855.6866\n",
      "Epoch 988/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 3462.9676 - val_loss: 2849.7751\n",
      "Epoch 989/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 3460.3771 - val_loss: 2848.2869\n",
      "Epoch 990/1000\n",
      "7438/7438 [==============================] - 1s 120us/step - loss: 3458.7041 - val_loss: 2847.4515\n",
      "Epoch 991/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 3461.8017 - val_loss: 2870.4519\n",
      "Epoch 992/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 3461.2105 - val_loss: 2851.5641\n",
      "Epoch 993/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 3450.9209 - val_loss: 2840.7807\n",
      "Epoch 994/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 3453.9559 - val_loss: 2854.8098\n",
      "Epoch 995/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 3451.4428 - val_loss: 2839.5239\n",
      "Epoch 996/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 3442.6729 - val_loss: 2834.6483\n",
      "Epoch 997/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 3441.0022 - val_loss: 2837.6314\n",
      "Epoch 998/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 3451.3223 - val_loss: 2839.5546\n",
      "Epoch 999/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 3442.5017 - val_loss: 2842.8141\n",
      "Epoch 1000/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 3439.7253 - val_loss: 2830.3979\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1244175908>"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train2, Y_train2, epochs=1000, batch_size=128, validation_data=(X_val2, Y_val2), callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7438 samples, validate on 1312 samples\n",
      "Epoch 1/1000\n",
      "7438/7438 [==============================] - 1s 145us/step - loss: 3430.6397 - val_loss: 2828.2426\n",
      "Epoch 2/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 3429.6307 - val_loss: 2833.2691\n",
      "Epoch 3/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 3429.7407 - val_loss: 2827.7048\n",
      "Epoch 4/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 3431.5670 - val_loss: 2828.7025\n",
      "Epoch 5/1000\n",
      "7438/7438 [==============================] - 1s 122us/step - loss: 3442.5363 - val_loss: 2837.2230\n",
      "Epoch 6/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 3420.2714 - val_loss: 2827.5992\n",
      "Epoch 7/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 3421.7992 - val_loss: 2834.0681\n",
      "Epoch 8/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 3427.4089 - val_loss: 2819.6999\n",
      "Epoch 9/1000\n",
      "7438/7438 [==============================] - 1s 122us/step - loss: 3415.0119 - val_loss: 2819.0853\n",
      "Epoch 10/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 3411.8955 - val_loss: 2838.0954\n",
      "Epoch 11/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 3424.9153 - val_loss: 2814.8237\n",
      "Epoch 12/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 3417.3972 - val_loss: 2816.1545\n",
      "Epoch 13/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 3404.4521 - val_loss: 2819.6014\n",
      "Epoch 14/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 3406.0681 - val_loss: 2815.0694\n",
      "Epoch 15/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 3399.3838 - val_loss: 2819.5062\n",
      "Epoch 16/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 3402.6924 - val_loss: 2825.3218\n",
      "Epoch 17/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 3418.1406 - val_loss: 2820.6628\n",
      "Epoch 18/1000\n",
      "7438/7438 [==============================] - 1s 131us/step - loss: 3404.3995 - val_loss: 2816.2397\n",
      "Epoch 19/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 3395.5687 - val_loss: 2807.2218\n",
      "Epoch 20/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 3392.5571 - val_loss: 2809.5793\n",
      "Epoch 21/1000\n",
      "7438/7438 [==============================] - 1s 119us/step - loss: 3386.2324 - val_loss: 2806.8371\n",
      "Epoch 22/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 3387.4385 - val_loss: 2805.9052\n",
      "Epoch 23/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 3380.4109 - val_loss: 2802.4853\n",
      "Epoch 24/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 3379.8033 - val_loss: 2797.5666\n",
      "Epoch 25/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 3403.7940 - val_loss: 2801.8998\n",
      "Epoch 26/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 3385.5517 - val_loss: 2801.5374\n",
      "Epoch 27/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 3379.6254 - val_loss: 2802.2549\n",
      "Epoch 28/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 3386.1262 - val_loss: 2799.5843\n",
      "Epoch 29/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 3405.1684 - val_loss: 2798.0213\n",
      "Epoch 30/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 3375.8798 - val_loss: 2802.0529\n",
      "Epoch 31/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 3372.2200 - val_loss: 2796.0021\n",
      "Epoch 32/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 3369.7256 - val_loss: 2795.8791\n",
      "Epoch 33/1000\n",
      "7438/7438 [==============================] - 1s 131us/step - loss: 3363.9222 - val_loss: 2786.1335\n",
      "Epoch 34/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 3359.4602 - val_loss: 2789.2234\n",
      "Epoch 35/1000\n",
      "7438/7438 [==============================] - 1s 121us/step - loss: 3355.9907 - val_loss: 2782.1736\n",
      "Epoch 36/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 3351.7716 - val_loss: 2781.9568\n",
      "Epoch 37/1000\n",
      "7438/7438 [==============================] - 1s 116us/step - loss: 3354.3761 - val_loss: 2794.9574\n",
      "Epoch 38/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 3367.3073 - val_loss: 2789.4803\n",
      "Epoch 39/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 3353.5991 - val_loss: 2784.5547\n",
      "Epoch 40/1000\n",
      "7438/7438 [==============================] - 1s 131us/step - loss: 3355.6858 - val_loss: 2779.4578\n",
      "Epoch 41/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 3342.0665 - val_loss: 2775.8547\n",
      "Epoch 42/1000\n",
      "7438/7438 [==============================] - 1s 122us/step - loss: 3345.9670 - val_loss: 2779.1335\n",
      "Epoch 43/1000\n",
      "7438/7438 [==============================] - 1s 132us/step - loss: 3336.9269 - val_loss: 2778.7746\n",
      "Epoch 44/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 3336.2936 - val_loss: 2771.8546\n",
      "Epoch 45/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 3357.8905 - val_loss: 2770.9279\n",
      "Epoch 46/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 3341.2008 - val_loss: 2774.4129\n",
      "Epoch 47/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 3328.3502 - val_loss: 2767.6180\n",
      "Epoch 48/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 3327.2901 - val_loss: 2767.4911\n",
      "Epoch 49/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 3324.5549 - val_loss: 2767.0080\n",
      "Epoch 50/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 3378.9337 - val_loss: 2765.0358\n",
      "Epoch 51/1000\n",
      "7438/7438 [==============================] - 1s 131us/step - loss: 3321.8469 - val_loss: 2764.7178\n",
      "Epoch 52/1000\n",
      "7438/7438 [==============================] - 1s 119us/step - loss: 3318.5607 - val_loss: 2765.4372\n",
      "Epoch 53/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 3314.0691 - val_loss: 2762.1517\n",
      "Epoch 54/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 3313.6078 - val_loss: 2766.4104\n",
      "Epoch 55/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 3313.6414 - val_loss: 2763.5477\n",
      "Epoch 56/1000\n",
      "7438/7438 [==============================] - 1s 127us/step - loss: 3309.3226 - val_loss: 2760.6175\n",
      "Epoch 57/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 3306.8899 - val_loss: 2755.1485\n",
      "Epoch 58/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 3303.4695 - val_loss: 2753.8780\n",
      "Epoch 59/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 3301.7410 - val_loss: 2758.2945\n",
      "Epoch 60/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 3298.8248 - val_loss: 2749.9589\n",
      "Epoch 61/1000\n",
      "7438/7438 [==============================] - 1s 130us/step - loss: 3297.3197 - val_loss: 2755.2287\n",
      "Epoch 62/1000\n",
      "7438/7438 [==============================] - 1s 132us/step - loss: 3407.4916 - val_loss: 2950.6701\n",
      "Epoch 63/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 3469.9719 - val_loss: 2888.8568\n",
      "Epoch 64/1000\n",
      "7438/7438 [==============================] - 1s 126us/step - loss: 3423.8615 - val_loss: 2856.9770\n",
      "Epoch 65/1000\n",
      "7438/7438 [==============================] - 1s 129us/step - loss: 3422.5023 - val_loss: 2853.2190\n",
      "Epoch 66/1000\n",
      "7438/7438 [==============================] - 1s 124us/step - loss: 3409.0270 - val_loss: 2837.3353\n",
      "Epoch 67/1000\n",
      "7438/7438 [==============================] - 1s 123us/step - loss: 3413.3263 - val_loss: 2806.8689\n",
      "Epoch 68/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 3428.8772 - val_loss: 2817.4346\n",
      "Epoch 69/1000\n",
      "7438/7438 [==============================] - 1s 128us/step - loss: 3425.7582 - val_loss: 2810.6693\n",
      "Epoch 70/1000\n",
      "7438/7438 [==============================] - 1s 125us/step - loss: 3367.4651 - val_loss: 2829.8874\n",
      "Epoch 00070: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f123c672c88>"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train1, Y_train1, epochs=1000, batch_size=128, validation_data=(X_val1, Y_val1), callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('lstm_model1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pd1 = readdata(test_pd)\n",
    "test_pd2 = extract(test_pd1)\n",
    "x_test = parse2test2(test_pd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_test, std_test = np.mean(test_pd2[9,:]),  np.std(test_pd2[9,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 4500)"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def valid_test(x, mean, std, a):\n",
    "    lower = mean - a * std\n",
    "    upper = mean + a * std\n",
    "    for i in range(9):\n",
    "        if x[9,i] <= lower or x[9,i] >= upper:\n",
    "            return False\n",
    "    return True\n",
    "valid_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y2 = model.predict(x_test, verbose=0)\n",
    "result(pred_y2, 'res6')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
